# terraform-aws-github-backup v2 — Design Document

## Overview

A Terraform module that backs up all repositories in a GitHub organization to S3.
Designed to be deployed by the customer in their own AWS account — zero operational
dependency on InfraHouse.

**Key design decisions:**
- Fargate scheduled task (no always-on compute, no Lambda timeout limit)
- Customer creates their own GitHub App (no shared credentials, short-lived tokens)
- InfraHouse-published container image on public ECR
- S3 bucket with versioning and lifecycle policies for retention

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                      Customer AWS Account                        │
│                                                                  │
│  EventBridge Schedule (e.g. daily 2am)                           │
│        │                                                         │
│        ▼                                                         │
│  ECS Fargate Task                                                │
│  ┌─────────────────────────────────────────────────────┐         │
│  │  public.ecr.aws/infrahouse/github-backup:latest     │         │
│  │                                                     │         │
│  │  1. Read GitHub App private key from Secrets Manager│         │
│  │  2. Generate JWT, exchange for installation token   │         │
│  │  3. List all repos via GitHub API                   │         │
│  │  4. git clone --mirror each repo                    │         │
│  │  5. git bundle create                               │         │
│  │  6. Upload bundles to S3                            │         │
│  │  7. Report success/failure metrics to CloudWatch    │         │
│  └─────────────────────────────────────────────────────┘         │
│        │                                                         │
│        ▼                                                         │
│  S3 Bucket (versioned, lifecycle policies)                       │
│  └── github-backup/                                              │
│      └── 2026-02-10/                                             │
│          ├── repo-a.bundle                                       │
│          ├── repo-b.bundle                                       │
│          └── manifest.json                                       │
│                                                                  │
│  CloudWatch                                                      │
│  ├── Log Group: /ecs/github-backup                               │
│  ├── Metric: BackupSuccess (count of repos backed up)            │
│  ├── Metric: BackupFailure (count of repos failed)               │
│  └── Alarm: BackupTaskFailed (task exit code != 0)               │
│                                                                  │
│  SNS Topic (optional, for alarm notifications)                   │
└──────────────────────────────────────────────────────────────────┘
```

---

## Module Structure (Terraform files)

```
terraform-aws-github-backup/
├── .claude/
│   └── CODING_STANDARD.md
├── .github/workflows/
│   └── ...                        # CI/CD (same pattern as other IH modules)
├── .bumpversion.cfg
├── .editorconfig
├── .gitignore
├── .terraform-docs.yml
├── CONTRIBUTING.md
├── LICENSE                        # Apache-2.0
├── Makefile
├── README.md                      # auto-generated by terraform-docs
├── SECURITY.md
├── cliff.toml
├── renovate.json
├── requirements.txt
│
├── terraform.tf                   # required providers (aws ~> 6.0) & versions
├── variables.tf                   # input variables
├── outputs.tf                     # output values
├── locals.tf                      # local values
├── datasources.tf                 # data sources
│
├── s3.tf                          # Source S3 bucket via infrahouse/s3-bucket module
├── s3_replication.tf              # Raw replica bucket (with region), replication config, IAM
├── iam.tf                         # Task execution role, task role, policies
├── ecs.tf                         # ECS cluster, task definition, container def
├── eventbridge.tf                 # EventBridge schedule rule + target
├── cloudwatch.tf                  # Log group, metric alarms
├── security_group.tf              # Security group for Fargate task
│
├── container/                     # Source for the Docker image (separate repo?)
│   ├── Dockerfile
│   ├── backup.py
│   └── requirements.txt
│
├── test_data/
│   └── ...
└── tests/
    └── ...
```

**Note:** The `container/` directory is the source for the public ECR image.
It could live in this repo or in a separate `github-backup-runner` repo.
Having it here keeps everything together; having it separate decouples
image releases from module releases. Recommend: **separate repo**
(`infrahouse/github-backup`) for the image, same pattern as `openvpn-portal`.

---

## Variables (variables.tf)

```hcl
# ── Required ─────────────────────────────────────────────────────

variable "github_app_id" {
  description = "The GitHub App ID. Found in the App's settings page."
  type        = string
}

variable "github_app_key_secret_arn" {
  description = "ARN of the Secrets Manager secret containing the GitHub App private key (PEM)."
  type        = string
}

variable "github_app_installation_id" {
  description = "The installation ID of the GitHub App on the target organization."
  type        = string
}

variable "subnets" {
  description = "List of subnet IDs for the Fargate task. Must have internet access (public subnets with auto-assign public IP, or private subnets with NAT gateway)."
  type        = list(string)
}

# ── Optional ─────────────────────────────────────────────────────

variable "environment" {
  description = "Name of environment."
  type        = string
  default     = "development"
}

variable "service_name" {
  description = "Descriptive name of the service. Used for naming resources."
  type        = string
  default     = "github-backup"
}

variable "schedule_expression" {
  description = "EventBridge schedule expression for backup frequency."
  type        = string
  default     = "rate(1 day)"
}

variable "backup_retention_days" {
  description = "Number of days to retain backups in S3 before expiration. Set to 0 to disable expiration."
  type        = number
  default     = 90
}

variable "task_cpu" {
  description = "CPU units for the Fargate task (256, 512, 1024, 2048, 4096)."
  type        = number
  default     = 1024
}

variable "task_memory" {
  description = "Memory (MiB) for the Fargate task."
  type        = number
  default     = 2048
}

variable "task_ephemeral_storage_gb" {
  description = "Ephemeral storage (GiB) for the Fargate task. Min 21, max 200."
  type        = number
  default     = 50
}

variable "image_uri" {
  description = "Docker image URI for the backup runner. Defaults to the InfraHouse public ECR image."
  type        = string
  default     = "public.ecr.aws/infrahouse/github-backup:latest"
}

variable "image_tag" {
  description = "Docker image tag. Used only when image_uri is the default InfraHouse image."
  type        = string
  default     = "latest"
}

variable "s3_bucket_name" {
  description = "Name for the S3 backup bucket. If null, a name is auto-generated."
  type        = string
  default     = null
}

variable "s3_kms_key_arn" {
  description = "ARN of a KMS key to use for S3 server-side encryption. If null, uses AES256."
  type        = string
  default     = null
}

variable "assign_public_ip" {
  description = "Whether to assign a public IP to the Fargate task. Required if using public subnets without NAT."
  type        = bool
  default     = false
}

variable "sns_topic_arn" {
  description = "ARN of an SNS topic for alarm notifications. If null, no alarm notifications are sent."
  type        = string
  default     = null
}

variable "tags" {
  description = "Tags to apply to all resources."
  type        = map(string)
  default     = {}
}
```

---

## Outputs (outputs.tf)

```hcl
output "s3_bucket_name" {
  description = "Name of the S3 bucket where backups are stored."
  value       = aws_s3_bucket.backup.id
}

output "s3_bucket_arn" {
  description = "ARN of the S3 bucket where backups are stored."
  value       = aws_s3_bucket.backup.arn
}

output "ecs_cluster_arn" {
  description = "ARN of the ECS cluster."
  value       = aws_ecs_cluster.backup.arn
}

output "task_definition_arn" {
  description = "ARN of the ECS task definition."
  value       = aws_ecs_task_definition.backup.arn
}

output "task_role_arn" {
  description = "ARN of the IAM role used by the backup task."
  value       = aws_iam_role.task.arn
}

output "log_group_name" {
  description = "Name of the CloudWatch log group."
  value       = aws_cloudwatch_log_group.backup.name
}

output "schedule_rule_arn" {
  description = "ARN of the EventBridge schedule rule."
  value       = aws_cloudwatch_event_rule.backup.arn
}
```

---

## IAM Permissions (iam.tf)

### Task Execution Role (used by ECS agent)
- `logs:CreateLogStream`, `logs:PutLogEvents` — write to CloudWatch Logs
- `secretsmanager:GetSecretValue` — read the GitHub App private key
- `ecr:GetAuthorizationToken`, `ecr:BatchGetImage`, `ecr:GetDownloadUrlForLayer` — pull the image

### Task Role (used by the running container)
- `s3:PutObject`, `s3:PutObjectAcl` — upload backups to the bucket
- `s3:ListBucket` — list existing backups (for manifest/cleanup)
- `kms:GenerateDataKey`, `kms:Decrypt` — if S3 KMS encryption is used
- `cloudwatch:PutMetricData` — publish backup metrics
- `secretsmanager:GetSecretValue` — read the GitHub App private key (task also needs this)

---

## Container Image Design (backup.py)

### Dependencies
- `PyJWT` + `cryptography` — generate GitHub App JWT
- `requests` — GitHub API calls
- `boto3` — S3 upload, Secrets Manager, CloudWatch
- `git` (system package) — clone repos

### Flow

```python
#!/usr/bin/env python3
"""
GitHub organization backup runner.

Environment variables (set by ECS task definition):
  GITHUB_APP_ID            - GitHub App ID
  GITHUB_APP_INSTALLATION_ID - Installation ID on the target org
  GITHUB_APP_KEY_SECRET_ARN  - Secrets Manager ARN for the private key
  S3_BUCKET                - Target S3 bucket name
  AWS_REGION               - AWS region (auto-set by ECS)
"""

def main():
    # 1. Read private key from Secrets Manager
    private_key = get_secret(GITHUB_APP_KEY_SECRET_ARN)

    # 2. Generate JWT (valid for 10 minutes)
    jwt_token = create_jwt(GITHUB_APP_ID, private_key)

    # 3. Exchange JWT for installation access token (valid for 1 hour)
    access_token = get_installation_token(jwt_token, GITHUB_APP_INSTALLATION_ID)

    # 4. List all repositories accessible to the installation
    repos = list_repositories(access_token)

    # 5. For each repo:
    results = {"success": [], "failed": []}
    date_prefix = datetime.utcnow().strftime("%Y-%m-%d")

    for repo in repos:
        try:
            # Clone with --mirror to temp directory
            clone_mirror(repo, access_token, tmp_dir)

            # Create git bundle
            bundle_path = create_bundle(repo, tmp_dir)

            # Upload to S3: s3://bucket/github-backup/YYYY-MM-DD/org/repo.bundle
            upload_to_s3(bundle_path, S3_BUCKET, date_prefix, repo)

            results["success"].append(repo["full_name"])
        except Exception as e:
            results["failed"].append({"repo": repo["full_name"], "error": str(e)})
        finally:
            # Clean up temp directory for this repo (don't accumulate disk)
            cleanup(tmp_dir)

    # 6. Write manifest
    upload_manifest(S3_BUCKET, date_prefix, results)

    # 7. Publish CloudWatch metrics
    publish_metrics(len(results["success"]), len(results["failed"]))

    # 8. Exit with failure if any repo failed
    if results["failed"]:
        log_failures(results["failed"])
        sys.exit(1)
```

### Important implementation details:
- **Clone each repo sequentially and clean up immediately** — don't accumulate
  all mirrors on disk. This keeps ephemeral storage manageable.
- **Use `git clone --mirror` with token in URL** —
  `https://x-access-token:{token}@github.com/org/repo.git`
- **`git bundle create`** produces a single portable file that contains the
  full git history. It can be restored with `git clone path/to/repo.bundle`.
- **Manifest JSON** records what was backed up, timestamps, sizes, and failures.
  Useful for auditing and alerting.
- **Token refresh** — if backing up many repos, the 1-hour token might expire.
  Implement a token refresh check before each clone.

### Dockerfile

```dockerfile
FROM python:3.12-slim

RUN apt-get update && \
    apt-get install -y --no-install-recommends git && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY backup.py .

ENTRYPOINT ["python", "backup.py"]
```

---

## S3 Bucket Layout

```
s3://github-backup-{account_id}-{region}/
└── github-backup/
    ├── 2026-02-10/
    │   ├── manifest.json
    │   ├── infrahouse/
    │   │   ├── terraform-aws-vpc.bundle
    │   │   ├── terraform-aws-ecs.bundle
    │   │   └── ...
    │   └── tinyfish-ai/
    │       ├── backend.bundle
    │       ├── frontend.bundle
    │       └── ...
    ├── 2026-02-09/
    │   └── ...
    └── ...
```

Lifecycle policy expires objects after `backup_retention_days`.

---

## Customer Setup — README instructions

The README should walk the customer through:

1. **Create a GitHub App** in your organization:
    - Go to `https://github.com/organizations/{org}/settings/apps/new`
    - App name: anything (e.g., "GitHub Backup")
    - Homepage URL: anything
    - Uncheck Webhook (Active) — no webhook needed
    - Permissions → Repository permissions → Contents: **Read-only**
    - Permissions → Repository permissions → Metadata: **Read-only**
    - Where can this app be installed? → **Only on this account**
    - Create the app

2. **Note the App ID** from the app settings page.

3. **Generate a private key** → download the `.pem` file.

4. **Install the app** on your organization → grant access to All repositories
   (or select specific ones).

5. **Note the Installation ID** from the URL after installing
   (`https://github.com/organizations/{org}/settings/installations/{id}`).

6. **Store the private key** in AWS Secrets Manager:
   ```bash
   aws secretsmanager create-secret \
     --name github-backup/app-private-key \
     --secret-string file://path-to-downloaded-key.pem
   ```

7. **Deploy the module:**
   ```hcl
   module "github_backup" {
     source  = "registry.infrahouse.com/infrahouse/github-backup/aws"
     version = "~> 1.0"

     github_app_id              = "123456"
     github_app_installation_id = "78901234"
     github_app_key_secret_arn  = "arn:aws:secretsmanager:us-west-2:111111111111:secret:github-backup/app-private-key-AbCdEf"

     subnets = ["subnet-abc123", "subnet-def456"]

     # Optional
     schedule_expression    = "rate(1 day)"
     backup_retention_days  = 90
     task_ephemeral_storage_gb = 100  # increase for large repos
   }
   ```

---

## What to do with the old module

- **Archive** `terraform-aws-github-backup` (the current ASG-based module)
- **Archive** `terraform-aws-github-backup-configuration` (the client-side module)
- **Deregister** the InfraHouse GitHub Backup App from GitHub (after migrating TinyFish)
- **Shut down** the EC2 instances running the backup service
- Create the new module as a **fresh repo** (clean git history, v1.0.0)
  or repurpose the existing repo with a breaking major version bump

---

## Cross-Region Replication

Uses AWS provider v6 per-resource `region` argument — no aliased providers needed.

### Additional variables:

```hcl
variable "replica_region" {
  description = "AWS region for cross-region backup replication. If null, replication is disabled."
  type        = string
  default     = null
}
```

### When `replica_region` is set, the module creates:

- Source S3 bucket via `infrahouse/s3-bucket` module (handles versioning, lifecycle, encryption)
- Replica S3 bucket via raw `aws_s3_bucket` resources with `region` attribute (AWS provider v6)
- S3 replication configuration from source → replica
- IAM role for S3 replication
- Versioning on both buckets (required for replication)
- Lifecycle policies on both buckets (source via module, replica via raw resources)

### When `replica_region` is null:

- Source bucket only, via `infrahouse/s3-bucket` module
- No replica bucket, no replication config

### Provider requirement:

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.0"
    }
  }
  required_version = "~> 1.5"
}
```

---

## Open Questions

1. **Separate image repo or monorepo?** Recommendation: separate repo
   `infrahouse/github-backup` for the Docker image, following the
   `openvpn-portal` pattern.

2. **Should the module create the ECS cluster or accept an existing one?**
   Recommendation: create its own — this is a standalone utility, not part of
   an application platform. But accept an optional `ecs_cluster_arn` variable
   for customers who want to reuse an existing cluster.

3. **Multiple orgs?** Should one deployment be able to back up multiple GitHub
   orgs? For v1, keep it simple — one module instance per org. The customer
   can instantiate the module multiple times.

4. **Incremental backups?** `git bundle` supports incremental bundles using
   `--since` or ref exclusions. Worth considering for v2 if repos are very
   large, but overkill for v1.

5. **Backup verification?** A separate scheduled task that downloads a random
   bundle, clones from it, and verifies integrity. Nice for compliance. Could
   be v1.1.
