diff --git a/.bumpversion.cfg b/.bumpversion.cfg
index 16a0c98..e9042d2 100644
--- a/.bumpversion.cfg
+++ b/.bumpversion.cfg
@@ -1,5 +1,5 @@
 [bumpversion]
-current_version = 0.7.3
+current_version = 1.0.0
 commit = True
 tag = True
 tag_name = {new_version}
diff --git a/.claude/plans/terraform-aws-github-backup-v2.md b/.claude/plans/terraform-aws-github-backup-v2.md
new file mode 100644
index 0000000..ce8ddb0
--- /dev/null
+++ b/.claude/plans/terraform-aws-github-backup-v2.md
@@ -0,0 +1,539 @@
+# terraform-aws-github-backup v2 — Design Document
+
+## Overview
+
+A Terraform module that backs up all repositories in a GitHub organization to S3.
+Designed to be deployed by the customer in their own AWS account — zero operational
+dependency on InfraHouse.
+
+**Key design decisions:**
+- Fargate scheduled task (no always-on compute, no Lambda timeout limit)
+- Customer creates their own GitHub App (no shared credentials, short-lived tokens)
+- InfraHouse-published container image on public ECR
+- S3 bucket with versioning and lifecycle policies for retention
+
+---
+
+## Architecture
+
+```
+┌──────────────────────────────────────────────────────────────────┐
+│                      Customer AWS Account                        │
+│                                                                  │
+│  EventBridge Schedule (e.g. daily 2am)                           │
+│        │                                                         │
+│        ▼                                                         │
+│  ECS Fargate Task                                                │
+│  ┌─────────────────────────────────────────────────────┐         │
+│  │  public.ecr.aws/infrahouse/github-backup:latest     │         │
+│  │                                                     │         │
+│  │  1. Read GitHub App private key from Secrets Manager │         │
+│  │  2. Generate JWT, exchange for installation token   │         │
+│  │  3. List all repos via GitHub API                   │         │
+│  │  4. git clone --mirror each repo                    │         │
+│  │  5. git bundle create                               │         │
+│  │  6. Upload bundles to S3                            │         │
+│  │  7. Report success/failure metrics to CloudWatch    │         │
+│  └─────────────────────────────────────────────────────┘         │
+│        │                                                         │
+│        ▼                                                         │
+│  S3 Bucket (versioned, lifecycle policies)                       │
+│  └── github-backup/                                              │
+│      └── 2026-02-10/                                             │
+│          ├── repo-a.bundle                                       │
+│          ├── repo-b.bundle                                       │
+│          └── manifest.json                                       │
+│                                                                  │
+│  CloudWatch                                                      │
+│  ├── Log Group: /ecs/github-backup                               │
+│  ├── Metric: BackupSuccess (count of repos backed up)            │
+│  ├── Metric: BackupFailure (count of repos failed)               │
+│  └── Alarm: BackupTaskFailed (task exit code != 0)               │
+│                                                                  │
+│  SNS Topic (optional, for alarm notifications)                   │
+└──────────────────────────────────────────────────────────────────┘
+```
+
+---
+
+## Module Structure (Terraform files)
+
+```
+terraform-aws-github-backup/
+├── .claude/
+│   └── CODING_STANDARD.md
+├── .github/workflows/
+│   └── ...                        # CI/CD (same pattern as other IH modules)
+├── .bumpversion.cfg
+├── .editorconfig
+├── .gitignore
+├── .terraform-docs.yml
+├── CONTRIBUTING.md
+├── LICENSE                        # Apache-2.0
+├── Makefile
+├── README.md                      # auto-generated by terraform-docs
+├── SECURITY.md
+├── cliff.toml
+├── renovate.json
+├── requirements.txt
+│
+├── terraform.tf                   # required providers (aws ~> 6.0) & versions
+├── variables.tf                   # input variables
+├── outputs.tf                     # output values
+├── locals.tf                      # local values
+├── datasources.tf                 # data sources
+│
+├── s3.tf                          # Source S3 bucket via infrahouse/s3-bucket module
+├── s3_replication.tf              # Raw replica bucket (with region), replication config, IAM
+├── iam.tf                         # Task execution role, task role, policies
+├── ecs.tf                         # ECS cluster, task definition, container def
+├── eventbridge.tf                 # EventBridge schedule rule + target
+├── cloudwatch.tf                  # Log group, metric alarms
+├── security_group.tf              # Security group for Fargate task
+│
+├── container/                     # Source for the Docker image (separate repo?)
+│   ├── Dockerfile
+│   ├── backup.py
+│   └── requirements.txt
+│
+├── test_data/
+│   └── ...
+└── tests/
+    └── ...
+```
+
+**Note:** The `container/` directory is the source for the public ECR image.
+It could live in this repo or in a separate `github-backup-runner` repo.
+Having it here keeps everything together; having it separate decouples
+image releases from module releases. Recommend: **separate repo**
+(`infrahouse/github-backup`) for the image, same pattern as `openvpn-portal`.
+
+---
+
+## Variables (variables.tf)
+
+```hcl
+# ── Required ─────────────────────────────────────────────────────
+
+variable "github_app_id" {
+  description = "The GitHub App ID. Found in the App's settings page."
+  type        = string
+}
+
+variable "github_app_key_secret_arn" {
+  description = "ARN of the Secrets Manager secret containing the GitHub App private key (PEM)."
+  type        = string
+}
+
+variable "github_app_installation_id" {
+  description = "The installation ID of the GitHub App on the target organization."
+  type        = string
+}
+
+variable "subnets" {
+  description = "List of subnet IDs for the Fargate task. Must have internet access (public subnets with auto-assign public IP, or private subnets with NAT gateway)."
+  type        = list(string)
+}
+
+# ── Optional ─────────────────────────────────────────────────────
+
+variable "environment" {
+  description = "Name of environment."
+  type        = string
+  default     = "development"
+}
+
+variable "service_name" {
+  description = "Descriptive name of the service. Used for naming resources."
+  type        = string
+  default     = "github-backup"
+}
+
+variable "schedule_expression" {
+  description = "EventBridge schedule expression for backup frequency."
+  type        = string
+  default     = "rate(1 day)"
+}
+
+variable "backup_retention_days" {
+  description = "Number of days to retain backups in S3 before expiration. Set to 0 to disable expiration."
+  type        = number
+  default     = 90
+}
+
+variable "task_cpu" {
+  description = "CPU units for the Fargate task (256, 512, 1024, 2048, 4096)."
+  type        = number
+  default     = 1024
+}
+
+variable "task_memory" {
+  description = "Memory (MiB) for the Fargate task."
+  type        = number
+  default     = 2048
+}
+
+variable "task_ephemeral_storage_gb" {
+  description = "Ephemeral storage (GiB) for the Fargate task. Min 21, max 200."
+  type        = number
+  default     = 50
+}
+
+variable "image_uri" {
+  description = "Docker image URI for the backup runner. Defaults to the InfraHouse public ECR image."
+  type        = string
+  default     = "public.ecr.aws/infrahouse/github-backup:latest"
+}
+
+variable "image_tag" {
+  description = "Docker image tag. Used only when image_uri is the default InfraHouse image."
+  type        = string
+  default     = "latest"
+}
+
+variable "s3_bucket_name" {
+  description = "Name for the S3 backup bucket. If null, a name is auto-generated."
+  type        = string
+  default     = null
+}
+
+variable "s3_kms_key_arn" {
+  description = "ARN of a KMS key to use for S3 server-side encryption. If null, uses AES256."
+  type        = string
+  default     = null
+}
+
+variable "assign_public_ip" {
+  description = "Whether to assign a public IP to the Fargate task. Required if using public subnets without NAT."
+  type        = bool
+  default     = false
+}
+
+variable "sns_topic_arn" {
+  description = "ARN of an SNS topic for alarm notifications. If null, no alarm notifications are sent."
+  type        = string
+  default     = null
+}
+
+variable "tags" {
+  description = "Tags to apply to all resources."
+  type        = map(string)
+  default     = {}
+}
+```
+
+---
+
+## Outputs (outputs.tf)
+
+```hcl
+output "s3_bucket_name" {
+  description = "Name of the S3 bucket where backups are stored."
+  value       = aws_s3_bucket.backup.id
+}
+
+output "s3_bucket_arn" {
+  description = "ARN of the S3 bucket where backups are stored."
+  value       = aws_s3_bucket.backup.arn
+}
+
+output "ecs_cluster_arn" {
+  description = "ARN of the ECS cluster."
+  value       = aws_ecs_cluster.backup.arn
+}
+
+output "task_definition_arn" {
+  description = "ARN of the ECS task definition."
+  value       = aws_ecs_task_definition.backup.arn
+}
+
+output "task_role_arn" {
+  description = "ARN of the IAM role used by the backup task."
+  value       = aws_iam_role.task.arn
+}
+
+output "log_group_name" {
+  description = "Name of the CloudWatch log group."
+  value       = aws_cloudwatch_log_group.backup.name
+}
+
+output "schedule_rule_arn" {
+  description = "ARN of the EventBridge schedule rule."
+  value       = aws_cloudwatch_event_rule.backup.arn
+}
+```
+
+---
+
+## IAM Permissions (iam.tf)
+
+### Task Execution Role (used by ECS agent)
+- `logs:CreateLogStream`, `logs:PutLogEvents` — write to CloudWatch Logs
+- `secretsmanager:GetSecretValue` — read the GitHub App private key
+- `ecr:GetAuthorizationToken`, `ecr:BatchGetImage`, `ecr:GetDownloadUrlForLayer` — pull the image
+
+### Task Role (used by the running container)
+- `s3:PutObject`, `s3:PutObjectAcl` — upload backups to the bucket
+- `s3:ListBucket` — list existing backups (for manifest/cleanup)
+- `kms:GenerateDataKey`, `kms:Decrypt` — if S3 KMS encryption is used
+- `cloudwatch:PutMetricData` — publish backup metrics
+- `secretsmanager:GetSecretValue` — read the GitHub App private key (task also needs this)
+
+---
+
+## Container Image Design (backup.py)
+
+### Dependencies
+- `PyJWT` + `cryptography` — generate GitHub App JWT
+- `requests` — GitHub API calls
+- `boto3` — S3 upload, Secrets Manager, CloudWatch
+- `git` (system package) — clone repos
+
+### Flow
+
+```python
+#!/usr/bin/env python3
+"""
+GitHub organization backup runner.
+
+Environment variables (set by ECS task definition):
+  GITHUB_APP_ID            - GitHub App ID
+  GITHUB_APP_INSTALLATION_ID - Installation ID on the target org
+  GITHUB_APP_KEY_SECRET_ARN  - Secrets Manager ARN for the private key
+  S3_BUCKET                - Target S3 bucket name
+  AWS_REGION               - AWS region (auto-set by ECS)
+"""
+
+def main():
+    # 1. Read private key from Secrets Manager
+    private_key = get_secret(GITHUB_APP_KEY_SECRET_ARN)
+
+    # 2. Generate JWT (valid for 10 minutes)
+    jwt_token = create_jwt(GITHUB_APP_ID, private_key)
+
+    # 3. Exchange JWT for installation access token (valid for 1 hour)
+    access_token = get_installation_token(jwt_token, GITHUB_APP_INSTALLATION_ID)
+
+    # 4. List all repositories accessible to the installation
+    repos = list_repositories(access_token)
+
+    # 5. For each repo:
+    results = {"success": [], "failed": []}
+    date_prefix = datetime.utcnow().strftime("%Y-%m-%d")
+
+    for repo in repos:
+        try:
+            # Clone with --mirror to temp directory
+            clone_mirror(repo, access_token, tmp_dir)
+
+            # Create git bundle
+            bundle_path = create_bundle(repo, tmp_dir)
+
+            # Upload to S3: s3://bucket/github-backup/YYYY-MM-DD/org/repo.bundle
+            upload_to_s3(bundle_path, S3_BUCKET, date_prefix, repo)
+
+            results["success"].append(repo["full_name"])
+        except Exception as e:
+            results["failed"].append({"repo": repo["full_name"], "error": str(e)})
+        finally:
+            # Clean up temp directory for this repo (don't accumulate disk)
+            cleanup(tmp_dir)
+
+    # 6. Write manifest
+    upload_manifest(S3_BUCKET, date_prefix, results)
+
+    # 7. Publish CloudWatch metrics
+    publish_metrics(len(results["success"]), len(results["failed"]))
+
+    # 8. Exit with failure if any repo failed
+    if results["failed"]:
+        log_failures(results["failed"])
+        sys.exit(1)
+```
+
+### Important implementation details:
+- **Clone each repo sequentially and clean up immediately** — don't accumulate
+  all mirrors on disk. This keeps ephemeral storage manageable.
+- **Use `git clone --mirror` with token in URL** —
+  `https://x-access-token:{token}@github.com/org/repo.git`
+- **`git bundle create`** produces a single portable file that contains the
+  full git history. It can be restored with `git clone path/to/repo.bundle`.
+- **Manifest JSON** records what was backed up, timestamps, sizes, and failures.
+  Useful for auditing and alerting.
+- **Token refresh** — if backing up many repos, the 1-hour token might expire.
+  Implement a token refresh check before each clone.
+
+### Dockerfile
+
+```dockerfile
+FROM python:3.12-slim
+
+RUN apt-get update && \
+    apt-get install -y --no-install-recommends git && \
+    rm -rf /var/lib/apt/lists/*
+
+WORKDIR /app
+
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+
+COPY backup.py .
+
+ENTRYPOINT ["python", "backup.py"]
+```
+
+---
+
+## S3 Bucket Layout
+
+```
+s3://github-backup-{account_id}-{region}/
+└── github-backup/
+    ├── 2026-02-10/
+    │   ├── manifest.json
+    │   ├── infrahouse/
+    │   │   ├── terraform-aws-vpc.bundle
+    │   │   ├── terraform-aws-ecs.bundle
+    │   │   └── ...
+    │   └── tinyfish-ai/
+    │       ├── backend.bundle
+    │       ├── frontend.bundle
+    │       └── ...
+    ├── 2026-02-09/
+    │   └── ...
+    └── ...
+```
+
+Lifecycle policy expires objects after `backup_retention_days`.
+
+---
+
+## Customer Setup — README instructions
+
+The README should walk the customer through:
+
+1. **Create a GitHub App** in your organization:
+    - Go to `https://github.com/organizations/{org}/settings/apps/new`
+    - App name: anything (e.g., "GitHub Backup")
+    - Homepage URL: anything
+    - Uncheck Webhook (Active) — no webhook needed
+    - Permissions → Repository permissions → Contents: **Read-only**
+    - Permissions → Repository permissions → Metadata: **Read-only**
+    - Where can this app be installed? → **Only on this account**
+    - Create the app
+
+2. **Note the App ID** from the app settings page.
+
+3. **Generate a private key** → download the `.pem` file.
+
+4. **Install the app** on your organization → grant access to All repositories
+   (or select specific ones).
+
+5. **Note the Installation ID** from the URL after installing
+   (`https://github.com/organizations/{org}/settings/installations/{id}`).
+
+6. **Store the private key** in AWS Secrets Manager:
+   ```bash
+   aws secretsmanager create-secret \
+     --name github-backup/app-private-key \
+     --secret-string file://path-to-downloaded-key.pem
+   ```
+
+7. **Deploy the module:**
+   ```hcl
+   module "github_backup" {
+     source  = "registry.infrahouse.com/infrahouse/github-backup/aws"
+     version = "~> 1.0"
+
+     github_app_id              = "123456"
+     github_app_installation_id = "78901234"
+     github_app_key_secret_arn  = "arn:aws:secretsmanager:us-west-2:111111111111:secret:github-backup/app-private-key-AbCdEf"
+
+     subnets = ["subnet-abc123", "subnet-def456"]
+
+     # Optional
+     schedule_expression    = "rate(1 day)"
+     backup_retention_days  = 90
+     task_ephemeral_storage_gb = 100  # increase for large repos
+   }
+   ```
+
+---
+
+## What to do with the old module
+
+- **Archive** `terraform-aws-github-backup` (the current ASG-based module)
+- **Archive** `terraform-aws-github-backup-configuration` (the client-side module)
+- **Deregister** the InfraHouse GitHub Backup App from GitHub (after migrating TinyFish)
+- **Shut down** the EC2 instances running the backup service
+- Create the new module as a **fresh repo** (clean git history, v1.0.0)
+  or repurpose the existing repo with a breaking major version bump
+
+---
+
+## Cross-Region Replication
+
+Uses AWS provider v6 per-resource `region` argument — no aliased providers needed.
+
+### Additional variables:
+
+```hcl
+variable "replica_region" {
+  description = "AWS region for cross-region backup replication. If null, replication is disabled."
+  type        = string
+  default     = null
+}
+```
+
+### When `replica_region` is set, the module creates:
+
+- Source S3 bucket via `infrahouse/s3-bucket` module (handles versioning, lifecycle, encryption)
+- Replica S3 bucket via raw `aws_s3_bucket` resources with `region` attribute (AWS provider v6)
+- S3 replication configuration from source → replica
+- IAM role for S3 replication
+- Versioning on both buckets (required for replication)
+- Lifecycle policies on both buckets (source via module, replica via raw resources)
+
+### When `replica_region` is null:
+
+- Source bucket only, via `infrahouse/s3-bucket` module
+- No replica bucket, no replication config
+
+### Provider requirement:
+
+```hcl
+terraform {
+  required_providers {
+    aws = {
+      source  = "hashicorp/aws"
+      version = "~> 6.0"
+    }
+  }
+  required_version = "~> 1.5"
+}
+```
+
+---
+
+## Open Questions
+
+1. **Separate image repo or monorepo?** Recommendation: separate repo
+   `infrahouse/github-backup` for the Docker image, following the
+   `openvpn-portal` pattern.
+
+2. **Should the module create the ECS cluster or accept an existing one?**
+   Recommendation: create its own — this is a standalone utility, not part of
+   an application platform. But accept an optional `ecs_cluster_arn` variable
+   for customers who want to reuse an existing cluster.
+
+3. **Multiple orgs?** Should one deployment be able to back up multiple GitHub
+   orgs? For v1, keep it simple — one module instance per org. The customer
+   can instantiate the module multiple times.
+
+4. **Incremental backups?** `git bundle` supports incremental bundles using
+   `--since` or ref exclusions. Worth considering for v2 if repos are very
+   large, but overkill for v1.
+
+5. **Backup verification?** A separate scheduled task that downloads a random
+   bundle, clones from it, and verifies integrity. Nice for compliance. Could
+   be v1.1.
diff --git a/.gitignore b/.gitignore
index aac36ae..bdb5928 100644
--- a/.gitignore
+++ b/.gitignore
@@ -23,3 +23,5 @@ docs/_build/
 .terraform.lock.hcl
 terraform.tfvars
 /.pytest_cache/
+/.claude/*.local.json
+pytest-*-output.log
diff --git a/Makefile b/Makefile
index d7a9eba..35a854a 100644
--- a/Makefile
+++ b/Makefile
@@ -1,5 +1,10 @@
 .DEFAULT_GOAL := help
 
+TEST_REGION ?= us-west-2
+TEST_ROLE ?= arn:aws:iam::303467602807:role/github-backup-tester
+TEST_SELECTOR ?= tests/
+TEST_FILTER ?= "test_"
+
 define PRINT_HELP_PYSCRIPT
 import re, sys
 
@@ -11,7 +16,7 @@ for line in sys.stdin:
 endef
 export PRINT_HELP_PYSCRIPT
 
-help: install-hooks
+help:  ## Show this help message
 	@python -c "$$PRINT_HELP_PYSCRIPT" < Makefile
 
 .PHONY: install-hooks
@@ -20,46 +25,78 @@ install-hooks:  ## Install repo hooks
 	@test -d .git/hooks || (echo "Looks like you are not in a Git repo" ; exit 1)
 	@test -L .git/hooks/pre-commit || ln -fs ../../hooks/pre-commit .git/hooks/pre-commit
 	@chmod +x .git/hooks/pre-commit
+	@test -L .git/hooks/commit-msg || ln -fs ../../hooks/commit-msg .git/hooks/commit-msg
+	@chmod +x .git/hooks/commit-msg
 
+.PHONY: bootstrap
+bootstrap: install-hooks  ## Bootstrap the development environment
+	pip install -U "pip ~= 24.0"
+	pip install -U "setuptools ~= 75.0"
+	pip install -r requirements.txt
 
 .PHONY: test
 test:  ## Run tests on the module
-	pytest -xvvs tests/
-
-
-.PHONY: bootstrap
-bootstrap: ## bootstrap the development environment
-	pip install -U "pip ~= 23.1"
-	pip install -U "setuptools ~= 68.0"
-	pip install -r requirements.txt
+	pytest -xvvs \
+		--aws-region=${TEST_REGION} \
+		--test-role-arn=${TEST_ROLE} \
+		$(if ${TEST_FILTER},-k ${TEST_FILTER}) \
+		$(TEST_SELECTOR) \
+		2>&1 | tee pytest-`date +%Y%m%d-%H%M%S`-output.log
+
+.PHONY: test-keep
+test-keep:  ## Run tests and keep infrastructure for debugging
+	pytest -xvvs \
+		--aws-region=${TEST_REGION} \
+		--test-role-arn=${TEST_ROLE} \
+		--keep-after \
+		$(if ${TEST_FILTER},-k ${TEST_FILTER}) \
+		$(TEST_SELECTOR) \
+		2>&1 | tee pytest-`date +%Y%m%d-%H%M%S`-output.log
+
+.PHONY: test-clean
+test-clean:  ## Run tests and clean up all resources
+	pytest -xvvs \
+		--aws-region=${TEST_REGION} \
+		--test-role-arn=${TEST_ROLE} \
+		$(if ${TEST_FILTER},-k ${TEST_FILTER}) \
+		$(TEST_SELECTOR) \
+		2>&1 | tee pytest-`date +%Y%m%d-%H%M%S`-output.log
 
 .PHONY: clean
-clean: ## clean the repo from cruft
+clean:  ## Clean build artifacts and caches
 	rm -rf .pytest_cache
+	find . -type d -name __pycache__ -exec rm -rf {} +
+	find . -type f -name '*.pyc' -delete
 	find . -name '.terraform' -exec rm -fr {} +
+	find . -name '.terraform.lock.hcl' -delete
 
 .PHONY: fmt
 fmt: format
 
 .PHONY: format
-format:  ## Use terraform fmt to format all files in the repo
-	@echo "Formatting terraform files"
+format:  ## Format all code
 	terraform fmt -recursive
-	black tests
-
-define BROWSER_PYSCRIPT
-import os, webbrowser, sys
-
-from urllib.request import pathname2url
-
-webbrowser.open("file://" + pathname2url(os.path.abspath(sys.argv[1])))
-endef
-export BROWSER_PYSCRIPT
-
-BROWSER := python -c "$$BROWSER_PYSCRIPT"
-
-.PHONY: docs
-docs: ## generate Sphinx HTML documentation, including API docs
-	$(MAKE) -C docs clean
-	$(MAKE) -C docs html
-	$(BROWSER) docs/_build/html/index.html
+	black tests container
+
+.PHONY: lint
+lint:  ## Run linters in check mode
+	terraform fmt -check -recursive
+	black --check tests container
+
+.PHONY: release-patch
+release-patch:  ## Release a patch version
+	git-cliff --tag $$(bumpversion --dry-run --list patch | grep new_version | cut -d= -f2) -o CHANGELOG.md
+	bumpversion patch
+	git push && git push --tags
+
+.PHONY: release-minor
+release-minor:  ## Release a minor version
+	git-cliff --tag $$(bumpversion --dry-run --list minor | grep new_version | cut -d= -f2) -o CHANGELOG.md
+	bumpversion minor
+	git push && git push --tags
+
+.PHONY: release-major
+release-major:  ## Release a major version
+	git-cliff --tag $$(bumpversion --dry-run --list major | grep new_version | cut -d= -f2) -o CHANGELOG.md
+	bumpversion major
+	git push && git push --tags
diff --git a/README.md b/README.md
index b303782..50bcb90 100644
--- a/README.md
+++ b/README.md
@@ -1,114 +1,76 @@
 # terraform-aws-github-backup
 
-This module automates the deployment of infrastructure for
-the [InfraHouse GitHub Backup](https://github.com/apps/infrahouse-github-backup) application,
-which is designed to back up GitHub repositories.
+[![Need Help?](https://img.shields.io/badge/Need%20Help%3F-Contact%20Us-0066CC)](https://infrahouse.com/contact)
+[![Docs](https://img.shields.io/badge/docs-github.io-blue)](https://infrahouse.github.io/terraform-aws-github-backup/)
+[![Registry](https://img.shields.io/badge/Terraform-Registry-purple?logo=terraform)](https://registry.terraform.io/modules/infrahouse/github-backup/aws/latest)
+[![Release](https://img.shields.io/github/release/infrahouse/terraform-aws-github-backup.svg)](https://github.com/infrahouse/terraform-aws-github-backup/releases/latest)
+[![Security](https://img.shields.io/github/actions/workflow/status/infrahouse/terraform-aws-github-backup/vuln-scanner-pr.yml?label=Security)](https://github.com/infrahouse/terraform-aws-github-backup/actions/workflows/vuln-scanner-pr.yml)
+[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
 
-The instances within the Auto Scaling Group perform the following tasks:
-- Read app installations and their configurations.
-- Create an in-memory backup copy of the repositories.
-- Upload the backups to an S3 bucket specified in the installation configuration.
+[![AWS ECS](https://img.shields.io/badge/AWS-ECS-orange?logo=amazonecs)](https://aws.amazon.com/ecs/)
+[![AWS S3](https://img.shields.io/badge/AWS-S3-green?logo=amazons3)](https://aws.amazon.com/s3/)
 
-**Note:** The backup of a repository is stored on an ephemeral disk,
-meaning it is not retained after the instance is terminated. This design choice enhances safety and security.
+A Terraform module that backs up all repositories in a GitHub organization to S3
+using an ECS Fargate scheduled task. Designed to be deployed by the customer in
+their own AWS account with zero operational dependency on InfraHouse.
 
-On the client side, the backups are configured by
-the [github-backup-configuration](https://github.com/infrahouse/terraform-aws-github-backup-configuration)
-module.
+## Why This Module?
 
-## Usage
+- **No always-on compute** -- Fargate runs on a schedule, you only pay for backup time
+- **No Lambda timeout limits** -- large organizations with many repos back up without issues
+- **Customer-owned GitHub App** -- no shared credentials, short-lived tokens only
+- **Cross-region replication** -- S3 replication for disaster recovery
+- **Full git history** -- uses `git bundle` for complete, portable backups
+
+## Features
+
+- ECS Fargate scheduled task (EventBridge) for daily/custom-schedule backups
+- S3 bucket with versioning and configurable retention lifecycle
+- Cross-region S3 replication (AWS provider v6, no aliased providers)
+- CloudWatch Logs, metrics, and alarm on backup failure
+- Least-privilege IAM roles for task execution and task runtime
+- Customer creates and owns their own GitHub App (read-only access)
+
+## Quick Start
+
+1. **Create a GitHub App** in your organization (see [Getting Started](https://infrahouse.github.io/terraform-aws-github-backup/#getting-started))
+2. **Deploy the module** -- the module creates a Secrets Manager secret for the App private key:
 
 ```hcl
-module "terraform-aws-github-backup" {
+module "github_backup" {
   source  = "registry.infrahouse.com/infrahouse/github-backup/aws"
-  version = "0.7.3"
-
-  app_key_secret           = module.infrahouse-github-backup-app-key.secret_name
-  subnets                  = module.management.subnet_private_ids
-  instance_type            = "t3a.small"
-  environment              = var.environment
-  root_volume_size         = 60
-  smtp_credentials_secret = module.smtp_credentials.secret_name
+  version = "~> 1.0"
+
+  github_app_id              = "123456"
+  github_app_installation_id = "78901234"
+
+  alarm_emails                  = ["devops@example.com"]
+  github_app_key_secret_writers = [aws_iam_role.deployer.arn]
+  replica_region                = "us-east-1"
+  subnets                       = ["subnet-abc123", "subnet-def456"]
+
+  # Optional
+  schedule_expression  = "rate(1 day)"
+  backup_retention_days = 365
 }
 ```
-## Requirements
-
-| Name | Version |
-|------|---------|
-| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | ~> 1.5 |
-| <a name="requirement_aws"></a> [aws](#requirement\_aws) | ~> 5.11 |
-| <a name="requirement_random"></a> [random](#requirement\_random) | ~> 3.6 |
-
-## Providers
-
-| Name | Version |
-|------|---------|
-| <a name="provider_aws"></a> [aws](#provider\_aws) | ~> 5.11 |
-| <a name="provider_random"></a> [random](#provider\_random) | ~> 3.6 |
-| <a name="provider_tls"></a> [tls](#provider\_tls) | n/a |
-
-## Modules
-
-| Name | Source | Version |
-|------|--------|---------|
-| <a name="module_instance_profile"></a> [instance\_profile](#module\_instance\_profile) | registry.infrahouse.com/infrahouse/instance-profile/aws | 1.8.1 |
-| <a name="module_userdata"></a> [userdata](#module\_userdata) | registry.infrahouse.com/infrahouse/cloud-init/aws | 1.12.4 |
-
-## Resources
-
-| Name | Type |
-|------|------|
-| [aws_autoscaling_group.main](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_group) | resource |
-| [aws_key_pair.deployer](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/key_pair) | resource |
-| [aws_launch_template.github-backup](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/launch_template) | resource |
-| [aws_security_group.backend](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group) | resource |
-| [aws_vpc_security_group_egress_rule.backend_outgoing](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_security_group_egress_rule) | resource |
-| [aws_vpc_security_group_ingress_rule.backend_icmp](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_security_group_ingress_rule) | resource |
-| [aws_vpc_security_group_ingress_rule.backend_ssh_local](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc_security_group_ingress_rule) | resource |
-| [random_string.profile_suffix](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/string) | resource |
-| [tls_private_key.deployer](https://registry.terraform.io/providers/hashicorp/tls/latest/docs/resources/private_key) | resource |
-| [aws_ami.selected](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/ami) | data source |
-| [aws_ami.ubuntu_pro](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/ami) | data source |
-| [aws_caller_identity.current](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/caller_identity) | data source |
-| [aws_default_tags.provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/default_tags) | data source |
-| [aws_iam_policy_document.default_permissions](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |
-| [aws_region.current](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/region) | data source |
-| [aws_secretsmanager_secret.app_key_secret](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/secretsmanager_secret) | data source |
-| [aws_subnet.selected](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/subnet) | data source |
-| [aws_vpc.service](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/vpc) | data source |
-
-## Inputs
-
-| Name | Description | Type | Default | Required |
-|------|-------------|------|---------|:--------:|
-| <a name="input_ami"></a> [ami](#input\_ami) | Image for EC2 instances | `string` | `null` | no |
-| <a name="input_app_key_secret"></a> [app\_key\_secret](#input\_app\_key\_secret) | secret name where the GitHub PEM is stored. | `string` | n/a | yes |
-| <a name="input_asg_max_healthy_percentage"></a> [asg\_max\_healthy\_percentage](#input\_asg\_max\_healthy\_percentage) | Specifies the upper limit on the number of instances that are in the InService or Pending state with a healthy status during an instance replacement activity. | `number` | `100` | no |
-| <a name="input_asg_max_size"></a> [asg\_max\_size](#input\_asg\_max\_size) | Maximum number of instances in ASG | `number` | `1` | no |
-| <a name="input_asg_min_healthy_percentage"></a> [asg\_min\_healthy\_percentage](#input\_asg\_min\_healthy\_percentage) | Specifies the lower limit on the number of instances that must be in the InService state with a healthy status during an instance replacement activity. | `number` | `0` | no |
-| <a name="input_asg_min_size"></a> [asg\_min\_size](#input\_asg\_min\_size) | Minimum number of instances in ASG | `number` | `1` | no |
-| <a name="input_environment"></a> [environment](#input\_environment) | Name of environment | `string` | `"development"` | no |
-| <a name="input_instance_role_name"></a> [instance\_role\_name](#input\_instance\_role\_name) | If specified, the instance profile role will have this name. Otherwise, the role name will be generated. | `string` | `"infrahouse-github-backup"` | no |
-| <a name="input_instance_type"></a> [instance\_type](#input\_instance\_type) | EC2 instances type | `string` | `"t3.micro"` | no |
-| <a name="input_key_pair_name"></a> [key\_pair\_name](#input\_key\_pair\_name) | SSH keypair name to be deployed in EC2 instances | `string` | `null` | no |
-| <a name="input_max_instance_lifetime_days"></a> [max\_instance\_lifetime\_days](#input\_max\_instance\_lifetime\_days) | The maximum amount of time, in \_days\_, that an instance can be in service, values must be either equal to 0 or between 7 and 365 days. | `number` | `30` | no |
-| <a name="input_packages"></a> [packages](#input\_packages) | List of packages to install when the instances bootstraps. | `list(string)` | `[]` | no |
-| <a name="input_puppet_custom_facts"></a> [puppet\_custom\_facts](#input\_puppet\_custom\_facts) | A map of custom puppet facts | `any` | `{}` | no |
-| <a name="input_puppet_debug_logging"></a> [puppet\_debug\_logging](#input\_puppet\_debug\_logging) | Enable debug logging if true. | `bool` | `false` | no |
-| <a name="input_puppet_environmentpath"></a> [puppet\_environmentpath](#input\_puppet\_environmentpath) | A path for directory environments. | `string` | `"{root_directory}/environments"` | no |
-| <a name="input_puppet_hiera_config_path"></a> [puppet\_hiera\_config\_path](#input\_puppet\_hiera\_config\_path) | Path to hiera configuration file. | `string` | `"{root_directory}/environments/{environment}/hiera.yaml"` | no |
-| <a name="input_puppet_manifest"></a> [puppet\_manifest](#input\_puppet\_manifest) | Path to puppet manifest. By default ih-puppet will apply {root\_directory}/environments/{environment}/manifests/site.pp. | `string` | `null` | no |
-| <a name="input_puppet_module_path"></a> [puppet\_module\_path](#input\_puppet\_module\_path) | Path to common puppet modules. | `string` | `"{root_directory}/environments/{environment}/modules:{root_directory}/modules"` | no |
-| <a name="input_puppet_root_directory"></a> [puppet\_root\_directory](#input\_puppet\_root\_directory) | Path where the puppet code is hosted. | `string` | `"/opt/puppet-code"` | no |
-| <a name="input_root_volume_size"></a> [root\_volume\_size](#input\_root\_volume\_size) | Root volume size in EC2 instance in Gigabytes | `number` | `30` | no |
-| <a name="input_service_name"></a> [service\_name](#input\_service\_name) | Descriptive name of a service that will use this VPC | `string` | `"infrahouse-github-backup"` | no |
-| <a name="input_smtp_credentials_secret"></a> [smtp\_credentials\_secret](#input\_smtp\_credentials\_secret) | AWS secret name with SMTP credentials. The secret must contain a JSON with user and password keys. | `string` | `null` | no |
-| <a name="input_subnets"></a> [subnets](#input\_subnets) | Subnet ids where EC2 instances should be present | `list(string)` | n/a | yes |
-| <a name="input_tags"></a> [tags](#input\_tags) | Tags to apply to instances in the autoscaling group. | `map(string)` | <pre>{<br/>  "Name": "infrahouse-github-backup"<br/>}</pre> | no |
-| <a name="input_ubuntu_codename"></a> [ubuntu\_codename](#input\_ubuntu\_codename) | Ubuntu version codename for the backup runner | `string` | `"noble"` | no |
-
-## Outputs
-
-| Name | Description |
-|------|-------------|
-| <a name="output_instance_role_arn"></a> [instance\_role\_arn](#output\_instance\_role\_arn) | ARN of the GitHub Backup instance role. |
+
+3. **Store the App private key** in the secret created by the module (output: `github_app_key_secret_arn`)
+
+## Documentation
+
+- [Getting Started](https://infrahouse.github.io/terraform-aws-github-backup/#getting-started)
+- [Configuration](https://infrahouse.github.io/terraform-aws-github-backup/#configuration)
+- [Architecture](https://infrahouse.github.io/terraform-aws-github-backup/#architecture)
+- [Restoring from a Backup](https://infrahouse.github.io/terraform-aws-github-backup/#restoring-from-a-backup)
+
+<!-- BEGIN_TF_DOCS -->
+<!-- END_TF_DOCS -->
+
+## Contributing
+
+See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.
+
+## License
+
+[Apache 2.0](LICENSE)
diff --git a/datasources.tf b/datasources.tf
index 1d44955..8b828b9 100644
--- a/datasources.tf
+++ b/datasources.tf
@@ -5,78 +5,3 @@ data "aws_default_tags" "provider" {}
 data "aws_subnet" "selected" {
   id = var.subnets[0]
 }
-
-data "aws_vpc" "service" {
-  id = data.aws_subnet.selected.vpc_id
-}
-
-data "aws_ami" "selected" {
-  filter {
-    name = "image-id"
-    values = [
-      var.ami == null ? data.aws_ami.ubuntu_pro.id : var.ami
-    ]
-  }
-}
-
-data "aws_ami" "ubuntu_pro" {
-  most_recent = true
-
-  filter {
-    name   = "name"
-    values = [local.ami_name_pattern_pro]
-  }
-
-  filter {
-    name   = "architecture"
-    values = ["x86_64"]
-  }
-
-  filter {
-    name   = "virtualization-type"
-    values = ["hvm"]
-  }
-
-  filter {
-    name = "state"
-    values = [
-      "available"
-    ]
-  }
-
-  owners = ["099720109477"] # Canonical
-}
-
-data "aws_iam_policy_document" "default_permissions" {
-  statement {
-    actions = [
-      "sts:GetCallerIdentity",
-      "sts:AssumeRole"
-    ]
-    resources = [
-      "*"
-    ]
-  }
-  statement {
-    actions = [
-      "secretsmanager:GetSecretValue"
-    ]
-    resources = [
-      data.aws_secretsmanager_secret.app_key_secret.arn
-    ]
-  }
-  # Allow reading tags by ih-ec2 tags
-  # The "ec2:DescribeInstances" action doesn't support Conditions, so we have to use the wildcard
-  statement {
-    actions = [
-      "ec2:DescribeInstances",
-    ]
-    resources = [
-      "*"
-    ]
-  }
-}
-
-data "aws_secretsmanager_secret" "app_key_secret" {
-  name = var.app_key_secret
-}
diff --git a/docs/index.md b/docs/index.md
index ce510e9..256f68c 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -1,3 +1,238 @@
 # terraform-aws-github-backup
 
-Module to provision infrahouse-github-backup GitHub App.
+A Terraform module that backs up all repositories in a GitHub organization to S3
+using an ECS Fargate scheduled task.
+
+## Overview
+
+This module deploys a scheduled ECS Fargate task that:
+
+1. Authenticates to GitHub using a customer-owned GitHub App
+2. Lists all repositories accessible to the App installation
+3. Creates a `git bundle` (full mirror) of each repository
+4. Uploads the bundles to a versioned S3 bucket
+5. Writes a manifest and publishes CloudWatch metrics
+
+## Architecture
+
+```
+EventBridge Schedule (e.g. daily 2am)
+       |
+       v
+ECS Fargate Task
+  1. Read GitHub App private key from Secrets Manager
+  2. Generate JWT, exchange for installation token
+  3. List all repos via GitHub API
+  4. git clone --mirror each repo
+  5. git bundle create
+  6. Upload bundles to S3
+  7. Report success/failure metrics to CloudWatch
+
+S3 Bucket (versioned, lifecycle policies)
+  github-backup/
+    2026-02-10/
+      manifest.json
+      org-name/
+        repo-a.bundle
+        repo-b.bundle
+
+S3 Replica Bucket (cross-region)
+  Replicates all objects for disaster recovery
+```
+
+## Getting Started
+
+### Prerequisites
+
+- Terraform >= 1.5
+- AWS provider >= 6.0
+- A GitHub App installed on your organization with read-only repository access
+
+### 1. Create a GitHub App
+
+1. Go to your GitHub organization **Settings > Developer settings > GitHub Apps > New GitHub App**
+2. Set the following permissions:
+    - **Repository permissions > Contents**: Read-only
+    - **Repository permissions > Metadata**: Read-only
+3. Install the App on your organization
+4. Note the **App ID** (from App settings) and **Installation ID** (from the installation URL)
+5. Generate a private key (PEM file) and save it securely
+
+### 2. Deploy the Module
+
+```hcl
+module "github_backup" {
+  source  = "registry.infrahouse.com/infrahouse/github-backup/aws"
+  version = "~> 1.0"
+
+  github_app_id              = "123456"
+  github_app_installation_id = "78901234"
+
+  alarm_emails                  = ["devops@example.com"]
+  github_app_key_secret_writers = [aws_iam_role.deployer.arn]
+  replica_region                = "us-east-1"
+  subnets                       = ["subnet-abc123", "subnet-def456"]
+}
+```
+
+### 3. Store the App Private Key
+
+The module creates a Secrets Manager secret for the GitHub App private key.
+After deployment, write the PEM key into the secret:
+
+```bash
+aws secretsmanager put-secret-value \
+  --secret-id "$(terraform output -raw github_app_key_secret_arn)" \
+  --secret-string file://github-app.pem
+```
+
+The role(s) specified in `github_app_key_secret_writers` must be used for this
+operation. The Terraform caller role automatically gets admin access to the
+secret.
+
+## Configuration
+
+### Required Variables
+
+| Variable | Type | Description |
+|---|---|---|
+| `github_app_id` | `string` | The GitHub App ID. Found in the App's settings page. |
+| `github_app_installation_id` | `string` | The installation ID of the GitHub App on the target organization. |
+| `alarm_emails` | `list(string)` | Email addresses to receive CloudWatch alarm notifications. At least one required. |
+| `replica_region` | `string` | AWS region for cross-region backup replication. |
+| `github_app_key_secret_writers` | `list(string)` | IAM role ARNs allowed to write the GitHub App private key into the secret. At least one required. |
+| `subnets` | `list(string)` | Subnet IDs for the Fargate task. Must have internet access. |
+
+### Optional Variables
+
+| Variable | Type | Default | Description |
+|---|---|---|---|
+| `environment` | `string` | `"development"` | Name of environment. Lowercase, numbers, and underscores only. |
+| `service_name` | `string` | `"github-backup"` | Descriptive name for the service. Used for naming resources. |
+| `schedule_expression` | `string` | `"rate(1 day)"` | EventBridge schedule expression for backup frequency. |
+| `backup_retention_days` | `number` | `365` | Days to retain backups in S3. Set to 0 to disable expiration. |
+| `image_uri` | `string` | `"public.ecr.aws/infrahouse/github-backup:latest"` | Docker image URI for the backup runner. |
+| `s3_bucket_name` | `string` | `null` | Custom name for the S3 backup bucket. Auto-generated if null. |
+| `force_destroy` | `bool` | `false` | Allow destroying S3 buckets with objects. For testing only. |
+| `tags` | `map(string)` | `{}` | Tags to apply to all resources. |
+
+### Outputs
+
+| Output | Description |
+|---|---|
+| `s3_bucket_name` | Name of the S3 bucket where backups are stored. |
+| `s3_bucket_arn` | ARN of the S3 bucket where backups are stored. |
+| `github_app_key_secret_arn` | ARN of the Secrets Manager secret for the GitHub App private key. |
+| `ecs_cluster_arn` | ARN of the ECS cluster. |
+| `ecs_cluster_name` | Name of the ECS cluster. |
+| `task_definition_arn` | ARN of the ECS task definition. |
+| `task_role_arn` | ARN of the IAM role used by the backup task. |
+| `log_group_name` | Name of the CloudWatch log group. |
+| `schedule_rule_arn` | ARN of the EventBridge schedule rule. |
+| `security_group_id` | ID of the security group for the Fargate task. |
+
+## Key Design Decisions
+
+- **Fargate** -- no always-on compute, no Lambda timeout limits
+- **Customer-owned GitHub App** -- no shared credentials, short-lived tokens
+- **InfraHouse-published container image** on public ECR
+- **S3 with versioning** and lifecycle policies for retention
+- **Cross-region replication** for disaster recovery
+- **AWS provider v6** -- per-resource `region` attribute, no aliased providers
+- **Module-managed secret** -- the module creates and manages the Secrets Manager
+  secret with a fine-grained resource policy (admin/writers/readers separation)
+
+## Disaster Recovery
+
+### RPO and RTO
+
+| Metric | Value | Notes |
+|---|---|---|
+| **RPO** (Recovery Point Objective) | Up to the schedule interval (default: 24h) | Determined by `schedule_expression`. Worst case is one full interval of data loss. |
+| **RTO** (Recovery Time Objective) | Minutes per repository | Restoring a single repo from a bundle takes seconds. Full org restore depends on the number and size of repositories. |
+
+### Backup Storage
+
+Backups are stored in two locations:
+
+- **Primary bucket** -- in the region where the module is deployed
+- **Replica bucket** -- in `replica_region`, automatically synchronized via S3 cross-region replication
+
+Both buckets are versioned, so even if a backup is overwritten or deleted, previous versions are retained according to the lifecycle policy.
+
+### Restore Procedures
+
+Git bundles are portable and self-contained. Each bundle is a full mirror of the repository at the time of backup, including all branches, tags, and history.
+
+#### Restore a single repository
+
+```bash
+# Download the bundle from S3
+aws s3 cp s3://BUCKET/github-backup/2026-02-10/org-name/repo.bundle repo.bundle
+
+# Verify the bundle is valid
+git bundle verify repo.bundle
+
+# Clone from the bundle
+git clone repo.bundle repo-restored
+
+# Point the restored repo back to GitHub
+cd repo-restored
+git remote set-url origin git@github.com:org-name/repo.git
+git push --mirror origin
+```
+
+#### Restore from the replica region
+
+If the primary region is unavailable:
+
+```bash
+# Download from the replica bucket
+aws s3 cp s3://BUCKET-replica/github-backup/2026-02-10/org-name/repo.bundle repo.bundle \
+  --region REPLICA_REGION
+
+# Then follow the same restore steps above
+```
+
+#### Restore all repositories from a specific date
+
+```bash
+# List available backup dates
+aws s3 ls s3://BUCKET/github-backup/
+
+# Download all bundles for a specific date
+aws s3 cp s3://BUCKET/github-backup/2026-02-10/ ./restore/ --recursive
+
+# Check the manifest for details
+cat restore/manifest.json
+
+# Restore each bundle
+for bundle in restore/org-name/*.bundle; do
+  repo_name=$(basename "$bundle" .bundle)
+  git clone "$bundle" "restored/$repo_name"
+done
+```
+
+#### Add a bundle as a remote to an existing repo
+
+```bash
+git remote add backup repo.bundle
+git fetch backup
+```
+
+### Failure Scenarios
+
+| Scenario | Detection | Recovery |
+|---|---|---|
+| Single repo fails to back up | `backup-failure` CloudWatch alarm | Check logs, re-run task manually |
+| Task does not run | `task-not-running` CloudWatch alarm | Check EventBridge rule and ECS cluster |
+| Primary bucket unavailable | AWS region outage | Restore from replica bucket |
+| GitHub App key compromised | Revoke in GitHub App settings | Generate new key, update the Secrets Manager secret |
+| Backup corruption | Future: verification task ([#21](https://github.com/infrahouse/terraform-aws-github-backup/issues/21)) | Restore from a previous day's backup (S3 versioning) |
+
+## Requirements
+
+| Name | Version |
+|---|---|
+| terraform | ~> 1.5 |
+| aws | ~> 6.0 |
diff --git a/locals.tf b/locals.tf
index a0b3d2d..59d259d 100644
--- a/locals.tf
+++ b/locals.tf
@@ -1,19 +1,42 @@
 locals {
-  module_version = "0.7.3"
+  module_version            = "1.0.0"
+  # 1 vCPU / 2 GB — sufficient for sequential git-clone + bundle
+  # operations.  Cloning is mostly I/O-bound (network + disk), so
+  # additional CPU gives diminishing returns.
+  task_cpu    = 1024
+  task_memory = 2048
+
+  # 50 GB ephemeral storage — must be large enough to hold the
+  # biggest single repository mirror + its git bundle at the same
+  # time.  Each repo is cloned, bundled, uploaded, then cleaned up
+  # before the next one starts, so this only needs to fit one repo.
+  task_ephemeral_storage_gb = 50
+
+  # Auto-generate bucket name if not provided
+  bucket_name = (
+    var.s3_bucket_name != null
+    ? var.s3_bucket_name
+    : "${var.service_name}-${data.aws_caller_identity.current.account_id}-${data.aws_region.current.name}"
+  )
+
+  # Shared lifecycle rule for source and replica buckets.
+  # Keeps backup retention consistent across both regions.
+  backup_lifecycle_rule = {
+    id     = "expire-old-backups"
+    status = var.backup_retention_days > 0 ? "Enabled" : "Disabled"
+    prefix = "github-backup/"
+    days   = var.backup_retention_days > 0 ? var.backup_retention_days : 1
+  }
 
   default_module_tags = {
-    environment : var.environment
-    service : var.service_name
-    account : data.aws_caller_identity.current.account_id
-    created_by_module : "infrahouse/github-backup/aws"
+    environment       = var.environment
+    service           = var.service_name
+    account           = data.aws_caller_identity.current.account_id
+    created_by_module = "infrahouse/github-backup/aws"
   }
-  default_asg_tags = merge(
-    {
-      Name : "infrahouse-github-backup"
-      module_version : local.module_version
-    },
-    local.default_module_tags
-  )
-  ami_name_pattern_pro = "ubuntu-pro-server/images/hvm-ssd-gp3/ubuntu-${var.ubuntu_codename}-*"
 
+  all_tags = merge(
+    local.default_module_tags,
+    var.tags,
+  )
 }
diff --git a/main.tf b/main.tf
deleted file mode 100644
index 52387a1..0000000
--- a/main.tf
+++ /dev/null
@@ -1,142 +0,0 @@
-resource "aws_autoscaling_group" "main" {
-  name_prefix           = aws_launch_template.github-backup.name
-  min_size              = var.asg_min_size
-  max_size              = var.asg_max_size
-  vpc_zone_identifier   = var.subnets
-  max_instance_lifetime = var.max_instance_lifetime_days * 24 * 3600
-  instance_refresh {
-    strategy = "Rolling"
-    preferences {
-      min_healthy_percentage = var.asg_min_healthy_percentage
-    }
-    triggers = ["tag"]
-  }
-  mixed_instances_policy {
-    instances_distribution {
-      on_demand_base_capacity                  = 0
-      on_demand_percentage_above_base_capacity = 0
-    }
-    launch_template {
-      launch_template_specification {
-        launch_template_id = aws_launch_template.github-backup.id
-        version            = aws_launch_template.github-backup.latest_version
-      }
-    }
-
-  }
-  instance_maintenance_policy {
-    min_healthy_percentage = var.asg_min_healthy_percentage
-    max_healthy_percentage = var.asg_max_healthy_percentage
-  }
-  dynamic "tag" {
-    for_each = merge(
-      local.default_asg_tags,
-      var.tags,
-      data.aws_default_tags.provider.tags
-    )
-    content {
-      key                 = tag.key
-      value               = tag.value
-      propagate_at_launch = true
-
-    }
-  }
-
-  lifecycle {
-    create_before_destroy = true
-  }
-}
-
-resource "tls_private_key" "deployer" {
-  algorithm = "RSA"
-}
-
-resource "aws_key_pair" "deployer" {
-  key_name_prefix = "${var.service_name}-deployer-generated-"
-  public_key      = tls_private_key.deployer.public_key_openssh
-  tags            = local.default_module_tags
-}
-
-
-resource "aws_launch_template" "github-backup" {
-  name_prefix   = "infrahouse-github-backup"
-  image_id      = data.aws_ami.selected.id
-  instance_type = var.instance_type
-  user_data     = module.userdata.userdata
-  key_name      = var.key_pair_name != null ? var.key_pair_name : aws_key_pair.deployer.key_name
-  vpc_security_group_ids = concat(
-    [aws_security_group.backend.id],
-  )
-  iam_instance_profile {
-    arn = module.instance_profile.instance_profile_arn
-  }
-
-  block_device_mappings {
-    device_name = data.aws_ami.selected.root_device_name
-    ebs {
-      volume_size           = var.root_volume_size
-      delete_on_termination = true
-    }
-  }
-  metadata_options {
-    http_tokens            = "required"
-    instance_metadata_tags = "enabled"
-  }
-  tag_specifications {
-    resource_type = "volume"
-    tags = merge(
-      data.aws_default_tags.provider.tags,
-      local.default_module_tags
-    )
-  }
-  tag_specifications {
-    resource_type = "network-interface"
-    tags = merge(
-      data.aws_default_tags.provider.tags,
-      local.default_module_tags
-    )
-  }
-
-}
-
-module "userdata" {
-  source      = "registry.infrahouse.com/infrahouse/cloud-init/aws"
-  version     = "1.18.0"
-  environment = var.environment
-  role        = "infrahouse_github_backup"
-  custom_facts = merge(
-    {
-      "infrahouse-github-backup" : {
-        "app-key-url" : "secretsmanager://${data.aws_secretsmanager_secret.app_key_secret.name}"
-      }
-    },
-    var.smtp_credentials_secret != null ? {
-      postfix : {
-        smtp_credentials : var.smtp_credentials_secret
-      }
-    } : {},
-    var.puppet_custom_facts
-  )
-  puppet_debug_logging     = var.puppet_debug_logging
-  puppet_environmentpath   = var.puppet_environmentpath
-  puppet_hiera_config_path = var.puppet_hiera_config_path
-  puppet_module_path       = var.puppet_module_path
-  puppet_root_directory    = var.puppet_root_directory
-  puppet_manifest          = var.puppet_manifest
-  packages                 = var.packages
-  ubuntu_codename          = var.ubuntu_codename
-}
-
-resource "random_string" "profile_suffix" {
-  length  = 12
-  special = false
-  upper   = false
-}
-
-module "instance_profile" {
-  source       = "registry.infrahouse.com/infrahouse/instance-profile/aws"
-  version      = "1.8.1"
-  profile_name = "${var.service_name}-${random_string.profile_suffix.result}"
-  role_name    = var.instance_role_name
-  permissions  = data.aws_iam_policy_document.default_permissions.json
-}
diff --git a/mkdocs.yml b/mkdocs.yml
index f812eff..b01ed83 100644
--- a/mkdocs.yml
+++ b/mkdocs.yml
@@ -1,4 +1,4 @@
-site_name: InfraHouse terraform-aws-github-backup
+site_name: InfraHouse github-backup
 site_url: https://infrahouse.github.io/terraform-aws-github-backup/
 site_description: Terraform module documentation
 site_author: InfraHouse
@@ -61,6 +61,7 @@ markdown_extensions:
 
 plugins:
   - search
+  - glightbox
   - minify:
       minify_html: true
 
@@ -71,4 +72,4 @@ extra:
     - icon: fontawesome/solid/globe
       link: https://infrahouse.com
 
-copyright: Copyright &copy; 2024-2026 InfraHouse
\ No newline at end of file
+copyright: Copyright &copy; 2024-2026 InfraHouse
diff --git a/outputs.tf b/outputs.tf
index 4243c95..d4d1968 100644
--- a/outputs.tf
+++ b/outputs.tf
@@ -1,4 +1,49 @@
-output "instance_role_arn" {
-  description = "ARN of the GitHub Backup instance role."
-  value       = module.instance_profile.instance_role_arn
+output "s3_bucket_name" {
+  description = "Name of the S3 bucket where backups are stored."
+  value       = module.backup_bucket.bucket_name
+}
+
+output "s3_bucket_arn" {
+  description = "ARN of the S3 bucket where backups are stored."
+  value       = module.backup_bucket.bucket_arn
+}
+
+output "github_app_key_secret_arn" {
+  description = "ARN of the Secrets Manager secret for the GitHub App private key."
+  value       = module.github_app_key.secret_arn
+}
+
+output "ecs_cluster_arn" {
+  description = "ARN of the ECS cluster."
+  value       = aws_ecs_cluster.backup.arn
+}
+
+output "task_definition_arn" {
+  description = "ARN of the ECS task definition."
+  value       = aws_ecs_task_definition.backup.arn
+}
+
+output "task_role_arn" {
+  description = "ARN of the IAM role used by the backup task."
+  value       = aws_iam_role.task.arn
+}
+
+output "log_group_name" {
+  description = "Name of the CloudWatch log group."
+  value       = aws_cloudwatch_log_group.backup.name
+}
+
+output "schedule_rule_arn" {
+  description = "ARN of the EventBridge schedule rule."
+  value       = aws_cloudwatch_event_rule.backup.arn
+}
+
+output "ecs_cluster_name" {
+  description = "Name of the ECS cluster."
+  value       = aws_ecs_cluster.backup.name
+}
+
+output "security_group_id" {
+  description = "ID of the security group for the Fargate task."
+  value       = aws_security_group.backup.id
 }
diff --git a/requirements.txt b/requirements.txt
index 082571a..6456b06 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,2 +1,8 @@
-pytest-infrahouse ~= 0.9
-infrahouse-core ~= 0.16
+pytest-infrahouse ~= 0.24
+infrahouse-core ~= 0.22
+
+# Documentation dependencies
+diagrams ~= 0.25
+mkdocs-material ~= 9.7
+mkdocs-minify-plugin ~= 0.8
+mkdocs-glightbox ~= 0.4
diff --git a/security_group.tf b/security_group.tf
index 7e2abf1..376bf5b 100644
--- a/security_group.tf
+++ b/security_group.tf
@@ -1,54 +1,26 @@
-resource "aws_security_group" "backend" {
-  description = "Backend security group for service ${var.service_name}"
+resource "aws_security_group" "backup" {
+  description = "Security group for ${var.service_name} Fargate task"
   name_prefix = "${var.service_name}-"
   vpc_id      = data.aws_subnet.selected.vpc_id
 
   tags = merge(
     {
-      Name : "${var.service_name} backend"
+      Name = "${var.service_name} fargate"
     },
-    local.default_module_tags
+    local.all_tags,
   )
 }
 
-resource "aws_vpc_security_group_ingress_rule" "backend_ssh_local" {
-  description       = "SSH access from the service ${var.service_name} VPC"
-  security_group_id = aws_security_group.backend.id
-  from_port         = 22
-  to_port           = 22
-  ip_protocol       = "tcp"
-  cidr_ipv4         = data.aws_vpc.service.cidr_block
-  tags = merge(
-    {
-      Name = "SSH local"
-    },
-    local.default_module_tags
-  )
-}
-
-resource "aws_vpc_security_group_ingress_rule" "backend_icmp" {
-  description       = "Allow all ICMP traffic"
-  security_group_id = aws_security_group.backend.id
-  from_port         = -1
-  to_port           = -1
-  ip_protocol       = "icmp"
-  cidr_ipv4         = "0.0.0.0/0"
-  tags = merge(
-    {
-      Name = "ICMP traffic"
-    },
-    local.default_module_tags
-  )
-}
-
-resource "aws_vpc_security_group_egress_rule" "backend_outgoing" {
-  security_group_id = aws_security_group.backend.id
+# Fargate tasks only need outbound access to GitHub,
+# S3, Secrets Manager, and CloudWatch. Allow all outbound.
+resource "aws_vpc_security_group_egress_rule" "all_outbound" {
+  security_group_id = aws_security_group.backup.id
   ip_protocol       = "-1"
   cidr_ipv4         = "0.0.0.0/0"
   tags = merge(
     {
       Name = "outgoing traffic"
     },
-    local.default_module_tags
+    local.all_tags,
   )
 }
diff --git a/terraform.tf b/terraform.tf
index 436b264..513e73e 100644
--- a/terraform.tf
+++ b/terraform.tf
@@ -3,11 +3,7 @@ terraform {
   required_providers {
     aws = {
       source  = "hashicorp/aws"
-      version = "~> 5.11"
-    }
-    random = {
-      source  = "hashicorp/random"
-      version = "~> 3.6"
+      version = "~> 6.0"
     }
   }
 }
diff --git a/test_data/main/datasources.tf b/test_data/main/datasources.tf
index 6fa7279..48f819d 100644
--- a/test_data/main/datasources.tf
+++ b/test_data/main/datasources.tf
@@ -1,5 +1,6 @@
 data "aws_caller_identity" "this" {}
 data "aws_region" "current" {}
-data "aws_availability_zones" "available" {
-  state = "available"
+
+data "aws_iam_role" "ecs_tester" {
+  name = "ecs-tester"
 }
diff --git a/test_data/main/main.tf b/test_data/main/main.tf
index 236c492..667ee86 100644
--- a/test_data/main/main.tf
+++ b/test_data/main/main.tf
@@ -1,26 +1,25 @@
-resource "aws_key_pair" "mediapc" {
-  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDpgAP1z1Lxg9Uv4tam6WdJBcAftZR4ik7RsSr6aNXqfnTj4civrhd/q8qMqF6wL//3OujVDZfhJcffTzPS2XYhUxh/rRVOB3xcqwETppdykD0XZpkHkc8XtmHpiqk6E9iBI4mDwYcDqEg3/vrDAGYYsnFwWmdDinxzMH1Gei+NPTmTqU+wJ1JZvkw3WBEMZKlUVJC/+nuv+jbMmCtm7sIM4rlp2wyzLWYoidRNMK97sG8+v+mDQol/qXK3Fuetj+1f+vSx2obSzpTxL4RYg1kS6W1fBlSvstDV5bQG4HvywzN5Y8eCpwzHLZ1tYtTycZEApFdy+MSfws5vPOpggQlWfZ4vA8ujfWAF75J+WABV4DlSJ3Ng6rLMW78hVatANUnb9s4clOS8H6yAjv+bU3OElKBkQ10wNneoFIMOA3grjPvPp5r8dI0WDXPIznJThDJO5yMCy3OfCXlu38VDQa1sjVj1zAPG+Vn2DsdVrl50hWSYSB17Zww0MYEr8N5rfFE= aleks@MediaPC"
-}
-
-resource "aws_key_pair" "black-mbp" {
-  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDBVMh/uBvxKF88z0VxbFYJwhGJklVWf90HJOiESQetC8AJXx6M0x9faPiK5z/SsFjNerCU9TwUZzEgLudB3OWm/X8BChGH3r1g5MsP3FpCd2UCQGu5/0jdX60TePhQ+4SVuoYpjaKIKhulzKM+lEcsJHIk+pM+cKA9yCt4rWghgp7OLXAJE2cA0qy0vv/DytReHoEPFFFtrKUSltmQhu1ggGXH+5pb7kFx2GWLElhVAeG0d+mJdRUXXnDzqjGvW2IrmOAcKJXkF5m9ITjKn55UiuZIPx4k/iLMQQ+am2F/VlttAdEl8Tgo27Q5UhqAH08sHrVnr1qciS8Rdavt8rNPSseFVh7e3wVvMBH4NvEd2gVPThssxlC7BjIfLQGb1jFiRdMHagbG4U4vtpr2pus2PnmcMOQwdC3WjvmyXHjCRQiS16FwJburRfBKGhQf30wjyzvyJ3PDMk4Sni/3Gl69TKb2s91Zq56yhCCrUjMhTtqjmdNvD8jEmIswV+fTyoU= aleks@Black-MBP"
-}
-
-resource "random_pet" "hostname" {
+resource "aws_ecr_repository" "backup" {
+  name         = "github-backup-test"
+  force_delete = true
 
-}
-
-module "appkey" {
-  source             = "registry.infrahouse.com/infrahouse/secret/aws"
-  version            = "~> 0.6"
-  secret_description = "GitHub App Key secret"
-  secret_name_prefix = "github-app"
-  secret_value       = "foo"
+  tags = {
+    Name = "github-backup-test"
+  }
 }
 
 module "main" {
-  source         = "../../"
-  subnets        = var.subnets
-  key_pair_name  = aws_key_pair.mediapc.key_name
-  app_key_secret = module.appkey.secret_name
-}
+  source = "../../"
+
+  github_app_id              = var.github_app_id
+  github_app_installation_id = var.github_app_installation_id
+  alarm_emails               = ["devops-test@infrahouse.com"]
+  # Use ecs-tester as the writer role (distinct from the Terraform caller
+  # role, github-backup-tester, which automatically gets admin access via
+  # the secret module's caller_role logic).
+  github_app_key_secret_writers = [data.aws_iam_role.ecs_tester.arn]
+  replica_region                = "us-east-1"
+  subnets                       = var.subnets
+  environment                   = "development"
+  force_destroy                 = true
+  image_uri                     = "${aws_ecr_repository.backup.repository_url}:latest"
+}
\ No newline at end of file
diff --git a/test_data/main/outputs.tf b/test_data/main/outputs.tf
index e69de29..a849291 100644
--- a/test_data/main/outputs.tf
+++ b/test_data/main/outputs.tf
@@ -0,0 +1,47 @@
+output "s3_bucket_name" {
+  value = module.main.s3_bucket_name
+}
+
+output "s3_bucket_arn" {
+  value = module.main.s3_bucket_arn
+}
+
+output "github_app_key_secret_arn" {
+  value = module.main.github_app_key_secret_arn
+}
+
+output "ecs_cluster_arn" {
+  value = module.main.ecs_cluster_arn
+}
+
+output "task_definition_arn" {
+  value = module.main.task_definition_arn
+}
+
+output "task_role_arn" {
+  value = module.main.task_role_arn
+}
+
+output "log_group_name" {
+  value = module.main.log_group_name
+}
+
+output "schedule_rule_arn" {
+  value = module.main.schedule_rule_arn
+}
+
+output "ecs_cluster_name" {
+  value = module.main.ecs_cluster_name
+}
+
+output "security_group_id" {
+  value = module.main.security_group_id
+}
+
+output "account_id" {
+  value = data.aws_caller_identity.this.account_id
+}
+
+output "ecr_repo_url" {
+  value = aws_ecr_repository.backup.repository_url
+}
diff --git a/test_data/main/providers.tf b/test_data/main/providers.tf
index 97831f5..8163386 100644
--- a/test_data/main/providers.tf
+++ b/test_data/main/providers.tf
@@ -1,12 +1,14 @@
 provider "aws" {
   region = var.region
-  assume_role {
-    role_arn = var.role_arn
+  dynamic "assume_role" {
+    for_each = var.role_arn != null ? [var.role_arn] : []
+    content {
+      role_arn = assume_role.value
+    }
   }
   default_tags {
     tags = {
-      "created_by" : "infrahouse/terraform-aws-github-backup" # GitHub repository that created a resource
+      "created_by" : "infrahouse/terraform-aws-github-backup"
     }
-
   }
 }
diff --git a/test_data/main/terraform.tf b/test_data/main/terraform.tf
index 981bc46..12e3228 100644
--- a/test_data/main/terraform.tf
+++ b/test_data/main/terraform.tf
@@ -3,11 +3,7 @@ terraform {
   required_providers {
     aws = {
       source  = "hashicorp/aws"
-      version = "~> 5.11"
-    }
-    cloudinit = {
-      source  = "hashicorp/cloudinit"
-      version = "~> 2.3"
+      version = "~> 6.0"
     }
   }
 }
diff --git a/test_data/main/variables.tf b/test_data/main/variables.tf
index baae29d..9ba7806 100644
--- a/test_data/main/variables.tf
+++ b/test_data/main/variables.tf
@@ -1,3 +1,20 @@
-variable "subnets" {}
-variable "region" {}
-variable "role_arn" {}
+variable "subnets" {
+  type = list(string)
+}
+
+variable "region" {
+  type = string
+}
+
+variable "role_arn" {
+  type    = string
+  default = null
+}
+
+variable "github_app_id" {
+  type = string
+}
+
+variable "github_app_installation_id" {
+  type = string
+}
diff --git a/test_data/service-network/datasources.tf b/test_data/service-network/datasources.tf
deleted file mode 100644
index 6fa7279..0000000
--- a/test_data/service-network/datasources.tf
+++ /dev/null
@@ -1,5 +0,0 @@
-data "aws_caller_identity" "this" {}
-data "aws_region" "current" {}
-data "aws_availability_zones" "available" {
-  state = "available"
-}
diff --git a/test_data/service-network/main.tf b/test_data/service-network/main.tf
deleted file mode 100644
index e9aaa98..0000000
--- a/test_data/service-network/main.tf
+++ /dev/null
@@ -1,41 +0,0 @@
-module "service-network" {
-  source                = "infrahouse/service-network/aws"
-  version               = "~> 2.3"
-  service_name          = "service-network"
-  vpc_cidr_block        = "10.1.0.0/16"
-  management_cidr_block = "10.1.0.0/16"
-  # must be enabled for EFS
-  enable_dns_hostnames = true
-  enable_dns_support   = true
-
-  subnets = [
-    {
-      cidr                    = "10.1.0.0/24"
-      availability-zone       = data.aws_availability_zones.available.names[0]
-      map_public_ip_on_launch = true
-      create_nat              = true
-      forward_to              = null
-    },
-    {
-      cidr                    = "10.1.1.0/24"
-      availability-zone       = data.aws_availability_zones.available.names[1]
-      map_public_ip_on_launch = true
-      create_nat              = true
-      forward_to              = null
-    },
-    {
-      cidr                    = "10.1.2.0/24"
-      availability-zone       = data.aws_availability_zones.available.names[0]
-      map_public_ip_on_launch = false
-      create_nat              = false
-      forward_to              = "10.1.0.0/24"
-    },
-    {
-      cidr                    = "10.1.3.0/24"
-      availability-zone       = data.aws_availability_zones.available.names[1]
-      map_public_ip_on_launch = false
-      create_nat              = false
-      forward_to              = "10.1.1.0/24"
-    }
-  ]
-}
diff --git a/test_data/service-network/outputs.tf b/test_data/service-network/outputs.tf
deleted file mode 100644
index ec5f485..0000000
--- a/test_data/service-network/outputs.tf
+++ /dev/null
@@ -1,11 +0,0 @@
-output "subnet_public_ids" {
-  value = module.service-network.subnet_public_ids
-}
-
-output "subnet_private_ids" {
-  value = module.service-network.subnet_private_ids
-}
-
-output "internet_gateway_id" {
-  value = module.service-network.internet_gateway_id
-}
diff --git a/test_data/service-network/providers.tf b/test_data/service-network/providers.tf
deleted file mode 100644
index 97831f5..0000000
--- a/test_data/service-network/providers.tf
+++ /dev/null
@@ -1,12 +0,0 @@
-provider "aws" {
-  region = var.region
-  assume_role {
-    role_arn = var.role_arn
-  }
-  default_tags {
-    tags = {
-      "created_by" : "infrahouse/terraform-aws-github-backup" # GitHub repository that created a resource
-    }
-
-  }
-}
diff --git a/test_data/service-network/terraform.tf b/test_data/service-network/terraform.tf
deleted file mode 100644
index 44aa1eb..0000000
--- a/test_data/service-network/terraform.tf
+++ /dev/null
@@ -1,9 +0,0 @@
-terraform {
-  //noinspection HILUnresolvedReference
-  required_providers {
-    aws = {
-      source  = "hashicorp/aws"
-      version = "~> 5.11"
-    }
-  }
-}
diff --git a/test_data/service-network/variables.tf b/test_data/service-network/variables.tf
deleted file mode 100644
index 0ddaa7f..0000000
--- a/test_data/service-network/variables.tf
+++ /dev/null
@@ -1,4 +0,0 @@
-variable "region" {
-}
-variable "role_arn" {
-}
diff --git a/tests/conftest.py b/tests/conftest.py
index 6151368..c614ad3 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -4,9 +4,14 @@ from infrahouse_core.logging import setup_logging
 
 # "303467602807" is our test account
 TEST_ACCOUNT = "303467602807"
-# TEST_ROLE_ARN = "arn:aws:iam::303467602807:role/ecs-tester"
 DEFAULT_PROGRESS_INTERVAL = 10
-UBUNTU_CODENAME = "jammy"
+TASK_RUN_TIMEOUT = 600  # seconds to wait for ECS task
+
+# InfraHouse GitHub Backup App (installed on the infrahouse org)
+GH_APP_ID = "1016509"
+GH_APP_INSTALLATION_ID = "55611573"
+GH_APP_PEM_SECRET_NAME = "github-backup-test-pem"
+GH_APP_PEM_SECRET_REGION = "us-west-1"
 
 LOG = logging.getLogger(__name__)
 TERRAFORM_ROOT_DIR = "test_data"
diff --git a/tests/test_module.py b/tests/test_module.py
index 7d40862..d55eb8a 100644
--- a/tests/test_module.py
+++ b/tests/test_module.py
@@ -1,26 +1,54 @@
 import json
+import time
+from base64 import b64decode
+from datetime import datetime, timezone
 from os import path as osp
+from subprocess import run
 from textwrap import dedent
 
+from infrahouse_core.aws.secretsmanager import Secret
+from infrahouse_core.timeout import timeout
 from pytest_infrahouse import terraform_apply
 
 from tests.conftest import (
+    GH_APP_ID,
+    GH_APP_INSTALLATION_ID,
+    GH_APP_PEM_SECRET_NAME,
+    GH_APP_PEM_SECRET_REGION,
     LOG,
+    TASK_RUN_TIMEOUT,
     TERRAFORM_ROOT_DIR,
 )
 
 
-def test_module(service_network, keep_after, test_role_arn, aws_region):
+def test_module(
+    service_network,
+    keep_after,
+    test_role_arn,
+    aws_region,
+    boto3_session,
+    cleanup_ecs_task_definitions,
+):
+    """
+    End-to-end test for the github-backup module.
+
+    1. Deploy the module (terraform apply).
+    2. Populate the module-created secret with a real GitHub App PEM key.
+    3. Run the ECS Fargate task.
+    4. Wait for task completion.
+    5. Verify backup bundles exist in the S3 bucket.
+    """
     subnet_public_ids = service_network["subnet_public_ids"]["value"]
 
-    # Create ECS with httpd container
     terraform_module_dir = osp.join(TERRAFORM_ROOT_DIR, "main")
     with open(osp.join(terraform_module_dir, "terraform.tfvars"), "w") as fp:
         fp.write(
             dedent(
                 f"""
-                region   = "{aws_region}"
-                subnets  = {json.dumps(subnet_public_ids)}
+                region                         = "{aws_region}"
+                subnets                        = {json.dumps(subnet_public_ids)}
+                github_app_id                  = "{GH_APP_ID}"
+                github_app_installation_id     = "{GH_APP_INSTALLATION_ID}"
                 """
             )
         )
@@ -28,7 +56,7 @@ def test_module(service_network, keep_after, test_role_arn, aws_region):
             fp.write(
                 dedent(
                     f"""
-                    role_arn      = "{test_role_arn}"
+                    role_arn = "{test_role_arn}"
                     """
                 )
             )
@@ -37,5 +65,190 @@ def test_module(service_network, keep_after, test_role_arn, aws_region):
         terraform_module_dir,
         destroy_after=not keep_after,
         json_output=True,
-    ) as tf_httpd_output:
-        LOG.info(json.dumps(tf_httpd_output, indent=4))
+    ) as tf_output:
+        LOG.info(json.dumps(tf_output, indent=4))
+
+        # ── Verify all expected outputs exist ───────────────────
+        assert "s3_bucket_name" in tf_output
+        assert "ecs_cluster_arn" in tf_output
+        assert "task_definition_arn" in tf_output
+        assert "task_role_arn" in tf_output
+        assert "log_group_name" in tf_output
+        assert "schedule_rule_arn" in tf_output
+        assert "github_app_key_secret_arn" in tf_output
+
+        bucket_name = tf_output["s3_bucket_name"]["value"]
+        assert bucket_name, "S3 bucket name should not be empty"
+
+        cluster_arn = tf_output["ecs_cluster_arn"]["value"]
+        assert cluster_arn.startswith("arn:aws:ecs:")
+
+        cluster_name = tf_output["ecs_cluster_name"]["value"]
+        task_definition_arn = tf_output["task_definition_arn"]["value"]
+        security_group_id = tf_output["security_group_id"]["value"]
+        module_secret_arn = tf_output["github_app_key_secret_arn"]["value"]
+
+        cleanup_ecs_task_definitions(
+            tf_output["task_definition_arn"]["value"].split("/")[-1].split(":")[0]
+        )
+
+        # ── Build and push the container image ────────────────
+        account_id = tf_output["account_id"]["value"]
+        ecr_repo_url = tf_output["ecr_repo_url"]["value"]
+
+        ecr = boto3_session.client("ecr", region_name=aws_region)
+        resp = ecr.get_authorization_token(registryIds=[account_id])
+        data = resp["authorizationData"][0]
+        userpass = b64decode(data["authorizationToken"]).decode("utf-8")
+        username, password = userpass.split(":", 1)
+        registry = data["proxyEndpoint"].replace("https://", "").replace("http://", "")
+
+        run(
+            ["docker", "login", "--username", username, "--password-stdin", registry],
+            input=password.encode("utf-8"),
+            check=True,
+        )
+
+        image_tag = f"{ecr_repo_url}:latest"
+        LOG.info("Building and pushing image: %s", image_tag)
+        run(
+            [
+                "docker",
+                "buildx",
+                "build",
+                "--platform",
+                "linux/amd64",
+                "-t",
+                image_tag,
+                "--push",
+                ".",
+            ],
+            cwd="container",
+            check=True,
+        )
+        LOG.info("Image pushed successfully: %s", image_tag)
+
+        # ── Populate the module-created secret with the PEM key ─
+        # Source secret is in the control repo's region (us-west-1).
+        pem_secret = Secret(
+            GH_APP_PEM_SECRET_NAME,
+            region=GH_APP_PEM_SECRET_REGION,
+            session=boto3_session,
+        )
+        pem_key = pem_secret.value
+        LOG.info(
+            "Read PEM key from %s in %s (%d bytes)",
+            GH_APP_PEM_SECRET_NAME,
+            GH_APP_PEM_SECRET_REGION,
+            len(pem_key),
+        )
+
+        module_secret = Secret(
+            module_secret_arn,
+            region=aws_region,
+            session=boto3_session,
+        )
+        module_secret.update(pem_key)
+        LOG.info("Wrote PEM key to module secret %s", module_secret_arn)
+
+        # ── Run the ECS task ────────────────────────────────────
+        ecs_client = boto3_session.client("ecs", region_name=aws_region)
+        run_response = ecs_client.run_task(
+            cluster=cluster_name,
+            taskDefinition=task_definition_arn,
+            launchType="FARGATE",
+            count=1,
+            networkConfiguration={
+                "awsvpcConfiguration": {
+                    "subnets": subnet_public_ids,
+                    "securityGroups": [security_group_id],
+                    "assignPublicIp": "ENABLED",
+                }
+            },
+        )
+        assert len(run_response["tasks"]) == 1, (
+            f"Expected 1 task, got {len(run_response['tasks'])}. "
+            f"Failures: {run_response.get('failures', [])}"
+        )
+
+        task_arn = run_response["tasks"][0]["taskArn"]
+        task_id = task_arn.split("/")[-1]
+        LOG.info("Started ECS task: %s", task_id)
+
+        # ── Wait for task to finish ─────────────────────────────
+        final_status = None
+        with timeout(TASK_RUN_TIMEOUT):
+            while True:
+                desc = ecs_client.describe_tasks(cluster=cluster_name, tasks=[task_arn])
+                task = desc["tasks"][0]
+                status = task["lastStatus"]
+                LOG.info("Task %s status: %s", task_id, status)
+
+                if status == "STOPPED":
+                    final_status = task
+                    break
+
+                time.sleep(15)
+
+        # ── Check task exit code ────────────────────────────────
+        container = final_status["containers"][0]
+        exit_code = container.get("exitCode", -1)
+        stop_reason = final_status.get("stoppedReason", "")
+        LOG.info(
+            "Task stopped. exit_code=%d reason=%s",
+            exit_code,
+            stop_reason,
+        )
+        assert exit_code == 0, (
+            f"Backup task failed with exit code {exit_code}. " f"Reason: {stop_reason}"
+        )
+
+        # ── Verify backup objects in S3 ─────────────────────────
+        s3_client = boto3_session.client("s3", region_name=aws_region)
+        today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
+        prefix = f"github-backup/{today}/"
+
+        response = s3_client.list_objects_v2(
+            Bucket=bucket_name,
+            Prefix=prefix,
+        )
+        assert (
+            "Contents" in response
+        ), f"No objects found under s3://{bucket_name}/{prefix}"
+
+        objects = response["Contents"]
+        LOG.info(
+            "Found %d objects under s3://%s/%s",
+            len(objects),
+            bucket_name,
+            prefix,
+        )
+        for obj in objects:
+            LOG.info("  %s  (%d bytes)", obj["Key"], obj["Size"])
+
+        # Expect at least one .bundle file and a manifest.json
+        bundle_keys = [o["Key"] for o in objects if o["Key"].endswith(".bundle")]
+        manifest_keys = [
+            o["Key"] for o in objects if o["Key"].endswith("manifest.json")
+        ]
+
+        assert len(bundle_keys) > 0, (
+            f"No .bundle files found under {prefix}. "
+            f"Keys: {[o['Key'] for o in objects]}"
+        )
+        assert (
+            len(manifest_keys) == 1
+        ), f"Expected exactly 1 manifest.json, found {len(manifest_keys)}"
+
+        # Verify manifest content
+        manifest_obj = s3_client.get_object(Bucket=bucket_name, Key=manifest_keys[0])
+        manifest = json.loads(manifest_obj["Body"].read().decode("utf-8"))
+        LOG.info("Manifest: %s", json.dumps(manifest, indent=2))
+
+        assert (
+            manifest["success_count"] > 0
+        ), "Expected at least one successful backup in manifest"
+        assert manifest["failure_count"] == 0, (
+            f"Backup had {manifest['failure_count']} failures: "
+            f"{manifest.get('failed', [])}"
+        )
diff --git a/variables.tf b/variables.tf
index 0e5a4ec..b0f084a 100644
--- a/variables.tf
+++ b/variables.tf
@@ -1,143 +1,160 @@
-variable "ami" {
-  description = "Image for EC2 instances"
+# ── Required ─────────────────────────────────────────────────────
+
+variable "github_app_id" {
+  description = "The GitHub App ID. Found in the App's settings page."
   type        = string
-  default     = null
 }
 
-variable "app_key_secret" {
-  description = "secret name where the GitHub PEM is stored."
+variable "github_app_installation_id" {
+  description = <<-EOT
+    The installation ID of the GitHub App on
+    the target organization.
+  EOT
   type        = string
 }
-variable "asg_min_size" {
-  description = "Minimum number of instances in ASG"
-  type        = number
-  default     = 1
-}
 
-variable "asg_max_size" {
-  description = "Maximum number of instances in ASG"
-  type        = number
-  default     = 1
+variable "alarm_emails" {
+  description = <<-EOT
+    List of email addresses to receive CloudWatch alarm
+    notifications. AWS will send confirmation emails that
+    must be accepted.
+  EOT
+  type        = list(string)
+
+  validation {
+    condition     = length(var.alarm_emails) > 0
+    error_message = "At least one email address must be provided for alarm notifications."
+  }
+
+  validation {
+    condition = alltrue([
+      for email in var.alarm_emails :
+      can(regex("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$", email))
+    ])
+    error_message = "All alarm_emails must be valid email addresses."
+  }
 }
 
-variable "asg_min_healthy_percentage" {
-  description = "Specifies the lower limit on the number of instances that must be in the InService state with a healthy status during an instance replacement activity."
-  type        = number
-  default     = 0
+variable "replica_region" {
+  description = <<-EOT
+    AWS region for cross-region backup replication.
+  EOT
+  type        = string
 }
 
-variable "asg_max_healthy_percentage" {
-  description = "Specifies the upper limit on the number of instances that are in the InService or Pending state with a healthy status during an instance replacement activity."
-  type        = number
-  default     = 100
+variable "github_app_key_secret_writers" {
+  description = <<-EOT
+    List of IAM role ARNs that are allowed to write
+    the GitHub App private key (PEM) into the secret
+    created by this module.
+  EOT
+  type        = list(string)
 
+  validation {
+    condition     = length(var.github_app_key_secret_writers) > 0
+    error_message = "At least one writer ARN is required to populate the secret."
+  }
 }
 
 variable "subnets" {
-  description = "Subnet ids where EC2 instances should be present"
+  description = <<-EOT
+    List of subnet IDs for the Fargate task.
+    Must have internet access (public subnets with
+    auto-assign public IP, or private subnets with
+    NAT gateway).
+  EOT
   type        = list(string)
+
+  validation {
+    condition     = length(var.subnets) >= 1
+    error_message = <<-EOT
+      At least one subnet is required.
+      Provided: ${length(var.subnets)}
+    EOT
+  }
 }
 
+# ── Optional ─────────────────────────────────────────────────────
+
 variable "environment" {
-  description = "Name of environment"
+  description = "Name of environment."
   type        = string
   default     = "development"
-}
 
-variable "instance_role_name" {
-  description = "If specified, the instance profile role will have this name. Otherwise, the role name will be generated."
-  type        = string
-  default     = "infrahouse-github-backup"
+  validation {
+    condition     = can(regex("^[a-z0-9_]+$", var.environment))
+    error_message = <<-EOT
+      environment must contain only lowercase letters,
+      numbers, and underscores (no hyphens).
+      Got: ${var.environment}
+    EOT
+  }
 }
 
-variable "instance_type" {
-  description = "EC2 instances type"
+variable "service_name" {
+  description = <<-EOT
+    Descriptive name of the service.
+    Used for naming resources.
+  EOT
   type        = string
-  default     = "t3.micro"
+  default     = "github-backup"
 }
 
-variable "key_pair_name" {
-  description = "SSH keypair name to be deployed in EC2 instances"
+variable "schedule_expression" {
+  description = <<-EOT
+    EventBridge schedule expression for backup frequency.
+    Examples: "rate(1 day)", "cron(0 2 * * ? *)"
+  EOT
   type        = string
-  default     = null
+  default     = "rate(1 day)"
 }
 
-variable "max_instance_lifetime_days" {
-  description = "The maximum amount of time, in _days_, that an instance can be in service, values must be either equal to 0 or between 7 and 365 days."
+variable "backup_retention_days" {
+  description = <<-EOT
+    Number of days to retain backups in S3 before
+    expiration. Set to 0 to disable expiration.
+  EOT
   type        = number
-  default     = 30
-}
-
-variable "packages" {
-  description = "List of packages to install when the instances bootstraps."
-  type        = list(string)
-  default     = []
-}
-
-variable "puppet_custom_facts" {
-  description = "A map of custom puppet facts"
-  type        = any
-  default     = {}
-}
-
-variable "puppet_debug_logging" {
-  description = "Enable debug logging if true."
-  type        = bool
-  default     = false
-}
-
-variable "puppet_environmentpath" {
-  description = "A path for directory environments."
-  default     = "{root_directory}/environments"
-}
-
-variable "puppet_hiera_config_path" {
-  description = "Path to hiera configuration file."
-  default     = "{root_directory}/environments/{environment}/hiera.yaml"
+  default     = 365
+
+  validation {
+    condition     = var.backup_retention_days >= 0
+    error_message = <<-EOT
+      backup_retention_days must be >= 0.
+      Got: ${var.backup_retention_days}
+    EOT
+  }
 }
 
-variable "puppet_manifest" {
-  description = "Path to puppet manifest. By default ih-puppet will apply {root_directory}/environments/{environment}/manifests/site.pp."
+variable "image_uri" {
+  description = <<-EOT
+    Docker image URI for the backup runner.
+    Defaults to the InfraHouse public ECR image.
+  EOT
   type        = string
-  default     = null
+  default     = "public.ecr.aws/infrahouse/github-backup:latest"
 }
 
-variable "puppet_module_path" {
-  description = "Path to common puppet modules."
-  default     = "{root_directory}/environments/{environment}/modules:{root_directory}/modules"
-}
-
-variable "puppet_root_directory" {
-  description = "Path where the puppet code is hosted."
-  default     = "/opt/puppet-code"
-}
-
-variable "root_volume_size" {
-  description = "Root volume size in EC2 instance in Gigabytes"
-  type        = number
-  default     = 30
-}
-variable "service_name" {
-  description = "Descriptive name of a service that will use this VPC"
+variable "s3_bucket_name" {
+  description = <<-EOT
+    Name for the S3 backup bucket.
+    If null, a name is auto-generated.
+  EOT
   type        = string
-  default     = "infrahouse-github-backup"
+  default     = null
 }
 
-variable "smtp_credentials_secret" {
-  description = "AWS secret name with SMTP credentials. The secret must contain a JSON with user and password keys."
-  type        = string
-  default     = null
+variable "force_destroy" {
+  description = <<-EOT
+    Allow destroying S3 buckets even when they contain
+    objects. Set to true only for testing.
+  EOT
+  type        = bool
+  default     = false
 }
 
 variable "tags" {
-  description = "Tags to apply to instances in the autoscaling group."
+  description = "Tags to apply to all resources."
   type        = map(string)
-  default = {
-    Name : "infrahouse-github-backup"
-  }
-}
-
-variable "ubuntu_codename" {
-  description = "Ubuntu version codename for the backup runner"
-  default     = "noble"
+  default     = {}
 }
